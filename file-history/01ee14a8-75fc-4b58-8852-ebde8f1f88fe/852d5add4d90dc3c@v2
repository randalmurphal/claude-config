# Orchestration MCP V2 - Research-Backed Architecture

## Research Findings (What Actually Works)

### From Anthropic's Multi-Agent Research System
- **Orchestrator-worker pattern**: Lead agent coordinates, subagents execute in parallel
- **Detailed task delegation**: Each subagent needs objective, output format, tool guidance, boundaries
- **Isolated context windows**: Subagents work independently, send only relevant info back
- **90.2% performance improvement** with multi-agent vs single-agent on complex tasks

### From METR Long-Task Research
- **Checkpoint-based validation** is critical for multi-hour tasks
- **Task length doubling every 7 months** - AI getting better at long tasks
- **30.4% success rate on complex autonomous tasks** - still needs human oversight
- **Validation must be real** - algorithmic eval overestimates capability

### Key Insight
> "Agents struggle to judge appropriate effort for different tasks, so scaling rules should be embedded in the prompts."

Translation: **The MCP must enforce scaling logic, not the orchestrator.**

---

## V2 Architecture: What to Build

### Core Principle
**MCP = Source of Truth + Validation Engine + Checkpoint Manager + Learning Store**

Not just metadata storage - actual intelligence about:
- What phase we're in
- What validation passed/failed
- What to do next
- What patterns worked before

### Three-Layer Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: ORCHESTRATOR (You/Claude)                  â”‚
â”‚ - Strategic decisions (what to build)               â”‚
â”‚ - Phase coordination (when to move forward)         â”‚
â”‚ - Recovery (what to do when things fail)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: MCP SERVER (Intelligence + State)          â”‚
â”‚ - Phase state machine (enforces workflow)           â”‚
â”‚ - Validation execution (runs pytest, linting)       â”‚
â”‚ - Checkpoint management (snapshots, rollback)       â”‚
â”‚ - Learning storage (Neo4j patterns/gotchas)         â”‚
â”‚ - Complexity scaling (adjust workflow to task size) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: WORKER AGENTS (Execution)                  â”‚
â”‚ - Architecture planner                              â”‚
â”‚ - Skeleton builders (parallel)                      â”‚
â”‚ - Implementation executors (parallel)               â”‚
â”‚ - Test writers                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase State Machine (MCP-Enforced)

```
START
  â†“
[ARCHITECTURE] - Understand what to build
  â†“ (validation: clear modules identified)
[SKELETON] - Create structure
  â†“ (validation: imports work, types defined)
[VALIDATE_SKELETON] - Checkpoint
  â†“ (validation: python -m compileall succeeds)
[IMPLEMENTATION] - Fill in TODOs
  â†“ (validation: manual checkpoint or quick test)
[TESTING] - Write tests
  â†“ (validation: tests exist, cover >90% of code)
[VALIDATE_IMPLEMENTATION] - Major checkpoint
  â†“ (validation: pytest passes, coverage met, linting passes)
[INTEGRATION] - If multi-module, test interactions
  â†“ (validation: integration tests pass)
[COMPLETE] - Task done
```

**Key: MCP enforces transitions.** You can't move to IMPLEMENTATION until SKELETON validation passes.

---

## MCP Tools We Actually Need

### Task Management
```python
# Start with complexity auto-detection
start_task(description, working_directory)
â†’ {task_id, complexity: "small|medium|large|massive", phases: [...]}

# Get current state and what to do next
get_task_state(task_id)
â†’ {current_phase, validation_status, next_action, blockers: [...]}

# Record completion and move to next phase
complete_phase(task_id, phase, artifacts: {...})
â†’ {next_phase, validation_required, estimated_time}
```

### Validation (THE CRITICAL MISSING PIECE)
```python
# Actually run validation, not self-report
validate_phase(task_id, phase, working_directory)
â†’ {
    passed: bool,
    checks_run: ["imports", "pytest", "coverage", "linting"],
    failures: [{check, reason, suggestion}],
    can_proceed: bool
}

# For skeleton: check imports, basic syntax
# For implementation: run pytest, check coverage, lint
# For integration: run integration tests
```

### Checkpoints (For 30+ Hour Tasks)
```python
# Save current state for rollback
create_checkpoint(task_id, checkpoint_name)
â†’ {checkpoint_id, snapshot_hash, files_captured}

# Rollback if things go wrong
rollback_to_checkpoint(task_id, checkpoint_id)
â†’ {restored_files, current_phase}

# List available checkpoints
list_checkpoints(task_id)
â†’ [{checkpoint_id, name, phase, timestamp, passed_validation}]
```

### Parallel Work Coordination
```python
# For multi-module tasks
create_work_chambers(task_id, modules: [str])
â†’ {chambers: [{module, worktree_path, agent_config}]}

# Get agent context for parallel work
get_agent_context(task_id, module, phase)
â†’ {objective, boundaries, tools, prior_learnings}

# Merge chambers back after validation passes
merge_chambers(task_id)
â†’ {conflicts: [...], resolution_suggestions: [...]}
```

### Learning Storage (Neo4j Usage)
```python
# Record what worked/failed
record_decision(task_id, question, chosen, rejected, rationale)
record_gotcha(task_id, phase, gotcha, resolution)
record_pattern(task_id, pattern_type, description, confidence)

# Query before starting similar work
get_relevant_learnings(description, phase)
â†’ {
    similar_tasks: [{description, outcomes, gotchas}],
    applicable_patterns: [{pattern, confidence, when_to_use}],
    time_estimate: {min, max, based_on: [...]}
}
```

### Complexity Scaling
```python
# MCP decides workflow based on task complexity
get_workflow_for_complexity(complexity)
â†’ {
    phases: [...],
    validation_gates: [...],
    parallel_allowed: bool,
    checkpoints_required: [phase_names],
    estimated_duration: {min, max}
}

# Small: Skip architecture, single chamber, lighter validation
# Medium: Standard phases, parallel if >3 modules
# Large: Full phases, mandatory checkpoints, aggressive parallelization
# Massive: Decompose into subtasks first
```

---

## Workflow for Different Task Sizes

### Small Tasks (1-3 files, <1 hour)
```
START â†’ SKELETON â†’ quick_test â†’ IMPLEMENTATION â†’ pytest â†’ COMPLETE
(Skip: architecture, integration, checkpoints)
```

### Medium Tasks (4-10 files, 1-4 hours)
```
START â†’ ARCHITECTURE â†’ SKELETON â†’ validate_imports
â†’ IMPLEMENTATION â†’ validate_pytest â†’ TESTING â†’ validate_coverage
â†’ COMPLETE
(Checkpoints: before implementation, after implementation)
```

### Large Tasks (10-30 files, 4-12 hours)
```
START â†’ ARCHITECTURE â†’ SKELETON (parallel) â†’ validate_skeleton
â†’ CHECKPOINT â†’ IMPLEMENTATION (parallel) â†’ validate_implementation
â†’ CHECKPOINT â†’ TESTING â†’ INTEGRATION â†’ validate_all â†’ COMPLETE
(Checkpoints: after each major phase)
```

### Massive Tasks (30+ files, 12-30+ hours)
```
START â†’ DECOMPOSE into subtasks
For each subtask:
  â†’ Run LARGE task workflow
  â†’ VALIDATE subtask checkpoint
  â†’ MERGE if passed, FIX if failed
â†’ FINAL INTEGRATION â†’ VALIDATE ALL â†’ COMPLETE
(Checkpoints: after every subtask, can rollback to any)
```

---

## What This Fixes

### 1. No More Self-Validation
**Before:** finalize_phase asks me if tests passed
**After:** validate_phase actually runs pytest and tells me results

### 2. Real Checkpoint System
**Before:** No way to save state for long tasks
**After:** Checkpoint after each phase, rollback if needed

### 3. MCP-Driven Workflow
**Before:** I manually coordinate everything
**After:** MCP tells me "skeleton validated, move to implementation"

### 4. Meaningful Neo4j Usage
**Before:** Just storing metadata
**After:**
- Pattern library (what works for similar tasks)
- Gotcha database (what typically goes wrong)
- Time estimation (based on actual history)

### 5. Scales Properly
**Before:** Same workflow for 1-file and 100-file tasks
**After:** MCP adjusts workflow based on complexity

---

## Implementation Priority

### Phase 1: Core Validation (Week 1)
- validate_phase with actual pytest execution
- Phase state machine enforcement
- Basic checkpoint system

### Phase 2: Parallel Work (Week 2)
- Chamber/worktree management
- Agent context generation
- Conflict detection on merge

### Phase 3: Learning System (Week 3)
- Neo4j pattern storage
- Similarity matching for context
- Time estimation from history

### Phase 4: Complexity Scaling (Week 4)
- Workflow adjustment logic
- Task decomposition
- Adaptive checkpointing

---

## What to Keep from Current System

âœ… **Keep:**
- Redis state management (works)
- Neo4j decision storage (good foundation)
- Basic MCP structure (solid)
- READY.md spec format (good for handoff)

âŒ **Rebuild:**
- finalize_phase (add real validation)
- Phase coordination (make MCP-driven)
- Agent context generation (needs Neo4j integration)
- Documentation (reflect reality)

ðŸ†• **Add:**
- validate_phase (actual test execution)
- Checkpoint system
- Complexity scaling logic
- Decomposition for massive tasks

---

## Success Metrics

For the system to be "bulletproof":
1. âœ… Can autonomously complete 3-hour task without intervention
2. âœ… Validation catches errors before moving to next phase
3. âœ… Checkpoints allow recovery from failures without starting over
4. âœ… Learning system provides relevant context from past tasks
5. âœ… Scales gracefully from 1-file to 100-file tasks
6. âœ… Clear progress tracking (user knows what's happening)
7. âœ… Handles failures gracefully (doesn't just crash)

---

## Questions to Resolve

1. **Validation strictness**: Block phase transition on test failure, or allow with warning?
2. **Checkpoint frequency**: Auto-checkpoint after each phase, or user-triggered?
3. **Parallel limits**: Max N parallel agents to avoid resource exhaustion?
4. **Decomposition threshold**: Auto-decompose at what file count/complexity?
5. **Neo4j query strategy**: Semantic search via PRISM or keyword matching?

---

## Next Steps

1. Review this architecture - does it match your vision?
2. Identify critical missing pieces
3. Prioritize what to build first
4. Start with validate_phase (biggest gap)
5. Test on real tasks, iterate