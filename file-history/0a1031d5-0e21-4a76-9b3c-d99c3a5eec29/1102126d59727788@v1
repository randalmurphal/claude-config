"""
Tests for TierDetector - GPU tier detection and model loading strategy.

Tests focus on BEHAVIOR that matters:
1. Does tier detection work with different VRAM scenarios?
2. Are manual tier overrides respected?
3. Does OOM handling trigger tier downgrade?
4. Does tier upgrade work when VRAM becomes available?
5. Does it crash on invalid config? (NO DEFAULTS)
6. Are tier requirements accurately reflected?
7. Does model loading succeed/fail appropriately per tier?
"""

import pytest
import torch
from unittest.mock import Mock, MagicMock, patch
from types import SimpleNamespace

from prism_mcp.models.tier_detector import (
    TierDetector,
    SystemTier,
    TierRequirements,
    get_tier_detector,
    load_embedder_models,
)


@pytest.fixture
def mock_config():
    """Create mock configuration for testing."""
    config = SimpleNamespace()
    config.models = SimpleNamespace()
    config.models.tier_strategy = "auto"
    config.models.manual_tier = None
    config.models.oom_retry_enabled = True
    config.models.oom_retry_max_attempts = 3
    config.models.oom_downgrade_tier = True
    return config


@pytest.fixture
def mock_config_manual():
    """Create mock configuration with manual tier override."""
    config = SimpleNamespace()
    config.models = SimpleNamespace()
    config.models.tier_strategy = "manual"
    config.models.manual_tier = 3  # SINGLE tier
    return config


class TestTierDetection:
    """Test tier detection logic."""

    def test_full_tier_selected_with_24gb(self, mock_config):
        """Full tier selected with 24GB VRAM."""
        with patch('torch.cuda.is_available', return_value=True), \
             patch('torch.cuda.get_device_properties') as mock_props:

            # Mock 24GB VRAM (24 * 1024^3 bytes)
            mock_device = Mock()
            mock_device.total_memory = 24 * (1024**3)
            mock_props.return_value = mock_device

            detector = TierDetector(mock_config)
            tier = detector.detect_tier()

            assert tier == SystemTier.FULL
            assert detector.vram_available_gb == 24.0

    def test_dual_tier_selected_with_16gb(self, mock_config):
        """Dual tier selected with 16GB VRAM."""
        with patch('torch.cuda.is_available', return_value=True), \
             patch('torch.cuda.get_device_properties') as mock_props:

            # Mock 16GB VRAM
            mock_device = Mock()
            mock_device.total_memory = 16 * (1024**3)
            mock_props.return_value = mock_device

            detector = TierDetector(mock_config)
            tier = detector.detect_tier()

            assert tier == SystemTier.DUAL
            assert detector.vram_available_gb == 16.0

    def test_single_tier_selected_with_8gb(self, mock_config):
        """Single tier selected with 8GB VRAM."""
        with patch('torch.cuda.is_available', return_value=True), \
             patch('torch.cuda.get_device_properties') as mock_props:

            # Mock 8GB VRAM
            mock_device = Mock()
            mock_device.total_memory = 8 * (1024**3)
            mock_props.return_value = mock_device

            detector = TierDetector(mock_config)
            tier = detector.detect_tier()

            assert tier == SystemTier.SINGLE
            assert detector.vram_available_gb == 8.0

    def test_cpu_tier_selected_with_insufficient_vram(self, mock_config):
        """CPU tier selected when GPU has insufficient VRAM."""
        with patch('torch.cuda.is_available', return_value=True), \
             patch('torch.cuda.get_device_properties') as mock_props:

            # Mock 4GB VRAM (insufficient for any GPU tier)
            mock_device = Mock()
            mock_device.total_memory = 4 * (1024**3)
            mock_props.return_value = mock_device

            detector = TierDetector(mock_config)
            tier = detector.detect_tier()

            assert tier == SystemTier.CPU
            assert detector.vram_available_gb == 4.0

    def test_cpu_tier_selected_without_gpu(self, mock_config):
        """CPU tier selected when no GPU available."""
        with patch('torch.cuda.is_available', return_value=False):

            detector = TierDetector(mock_config)
            tier = detector.detect_tier()

            assert tier == SystemTier.CPU
            assert detector.vram_available_gb == 0.0

    def test_vram_detection_failure_fallback(self, mock_config):
        """VRAM detection failure falls back to 0GB."""
        with patch('torch.cuda.is_available', return_value=True), \
             patch('torch.cuda.get_device_properties', side_effect=Exception("GPU error")):

            detector = TierDetector(mock_config)
            tier = detector.detect_tier()

            assert tier == SystemTier.CPU
            assert detector.vram_available_gb == 0.0

    def test_manual_tier_override(self, mock_config_manual):
        """Manual tier override respected."""
        detector = TierDetector(mock_config_manual)
        tier = detector.detect_tier()

        assert tier == SystemTier.SINGLE  # manual_tier = 3
        assert detector.current_tier == SystemTier.SINGLE

    def test_manual_tier_invalid_number_crashes(self, mock_config):
        """Invalid manual tier number crashes."""
        mock_config.models.tier_strategy = "manual"
        mock_config.models.manual_tier = 99  # Invalid

        detector = TierDetector(mock_config)

        with pytest.raises(RuntimeError, match="Invalid manual_tier: 99"):
            detector.detect_tier()

    def test_manual_tier_missing_config_crashes(self, mock_config):
        """Missing manual tier config crashes."""
        mock_config.models.tier_strategy = "manual"
        mock_config.models.manual_tier = None

        detector = TierDetector(mock_config)

        with pytest.raises(RuntimeError, match="Manual tier strategy requires manual_tier config"):
            detector.detect_tier()


class TestTierRequirements:
    """Test tier requirements are correctly defined."""

    def test_all_tiers_have_requirements(self):
        """All system tiers have requirements defined."""
        for tier in SystemTier:
            assert tier in TierDetector.TIER_REQUIREMENTS
            req = TierDetector.TIER_REQUIREMENTS[tier]
            assert isinstance(req, TierRequirements)
            assert hasattr(req, 'min_vram_gb')
            assert hasattr(req, 'models')
            assert hasattr(req, 'features')
            assert hasattr(req, 'description')

    def test_tier_memory_requirements_logical(self):
        """Tier memory requirements are in logical order."""
        requirements = TierDetector.TIER_REQUIREMENTS

        assert requirements[SystemTier.FULL].min_vram_gb >= 24.0
        assert requirements[SystemTier.DUAL].min_vram_gb >= 16.0
        assert requirements[SystemTier.SINGLE].min_vram_gb >= 8.0
        assert requirements[SystemTier.CPU].min_vram_gb == 0.0
        assert requirements[SystemTier.KEYWORD].min_vram_gb == 0.0

    def test_tier_models_logical(self):
        """Tier model requirements are logical."""
        requirements = TierDetector.TIER_REQUIREMENTS

        # FULL has all models
        assert "e5" in requirements[SystemTier.FULL].models
        assert "starcoder2" in requirements[SystemTier.FULL].models
        assert "bge" in requirements[SystemTier.FULL].models

        # DUAL has e5 + starcoder2
        assert "e5" in requirements[SystemTier.DUAL].models
        assert "starcoder2" in requirements[SystemTier.DUAL].models
        assert "bge" not in requirements[SystemTier.DUAL].models

        # SINGLE has only e5
        assert "e5" in requirements[SystemTier.SINGLE].models
        assert "starcoder2" not in requirements[SystemTier.SINGLE].models
        assert "bge" not in requirements[SystemTier.SINGLE].models

        # CPU has only e5
        assert "e5" in requirements[SystemTier.CPU].models
        assert "starcoder2" not in requirements[SystemTier.CPU].models
        assert "bge" not in requirements[SystemTier.CPU].models

        # KEYWORD has no models
        assert len(requirements[SystemTier.KEYWORD].models) == 0

    def test_tier_features_logical(self):
        """Tier feature flags are logical."""
        requirements = TierDetector.TIER_REQUIREMENTS

        # FULL has all features
        assert requirements[SystemTier.FULL].features["semantic_search"] is True
        assert requirements[SystemTier.FULL].features["code_patterns"] is True
        assert requirements[SystemTier.FULL].features["reranking"] is True

        # DUAL has semantic + code but no reranking
        assert requirements[SystemTier.DUAL].features["semantic_search"] is True
        assert requirements[SystemTier.DUAL].features["code_patterns"] is True
        assert requirements[SystemTier.DUAL].features["reranking"] is False

        # SINGLE has only semantic
        assert requirements[SystemTier.SINGLE].features["semantic_search"] is True
        assert requirements[SystemTier.SINGLE].features["code_patterns"] is False
        assert requirements[SystemTier.SINGLE].features["reranking"] is False

        # CPU has only semantic
        assert requirements[SystemTier.CPU].features["semantic_search"] is True
        assert requirements[SystemTier.CPU].features["code_patterns"] is False
        assert requirements[SystemTier.CPU].features["reranking"] is False

        # KEYWORD has no features
        assert requirements[SystemTier.KEYWORD].features["semantic_search"] is False
        assert requirements[SystemTier.KEYWORD].features["code_patterns"] is False
        assert requirements[SystemTier.KEYWORD].features["reranking"] is False


class TestOOMHandling:
    """Test OOM error handling."""

    def test_oom_triggers_downgrade(self, mock_config):
        """OOM error triggers tier downgrade."""
        detector = TierDetector(mock_config)
        detector.current_tier = SystemTier.FULL

        # Mock model loading to fail for DUAL but succeed for SINGLE
        # From FULL, tries: DUAL, SINGLE, CPU, KEYWORD
        def mock_attempt_load(tier):
            if tier == SystemTier.DUAL:
                return False
            elif tier == SystemTier.SINGLE:
                detector.current_tier = tier  # Mock sets current_tier on success
                return True
            return False

        with patch.object(detector, 'attempt_load_models', side_effect=mock_attempt_load):
            new_tier = detector.handle_oom_error()

            assert new_tier == SystemTier.SINGLE
            assert detector.current_tier == SystemTier.SINGLE

    def test_oom_cascades_to_cpu(self, mock_config):
        """Multiple OOM errors cascade to CPU tier."""
        detector = TierDetector(mock_config)
        detector.current_tier = SystemTier.FULL

        # Mock model loading to fail for DUAL and SINGLE but succeed for CPU
        # From FULL, tries: DUAL, SINGLE, CPU, KEYWORD
        def mock_attempt_load(tier):
            if tier in [SystemTier.DUAL, SystemTier.SINGLE]:
                return False
            elif tier == SystemTier.CPU:
                detector.current_tier = tier  # Mock sets current_tier on success
                return True
            return False

        with patch.object(detector, 'attempt_load_models', side_effect=mock_attempt_load):
            new_tier = detector.handle_oom_error()

            assert new_tier == SystemTier.CPU
            assert detector.current_tier == SystemTier.CPU

    def test_oom_complete_failure_crashes(self, mock_config):
        """Complete OOM failure crashes system."""
        detector = TierDetector(mock_config)
        detector.current_tier = SystemTier.FULL

        # Mock model loading to fail for all tiers
        with patch.object(detector, 'attempt_load_models', return_value=False):

            with pytest.raises(RuntimeError, match="Cannot recover from OOM error"):
                detector.handle_oom_error()


class TestTierUpgrade:
    """Test hot tier upgrade capability."""

    def test_tier_upgrade_when_vram_available(self, mock_config):
        """Tier upgrades when more VRAM becomes available."""
        detector = TierDetector(mock_config)
        detector.current_tier = SystemTier.SINGLE

        # Mock model loading to succeed for DUAL tier
        # From SINGLE, tries higher tiers in order: FULL, DUAL
        def mock_attempt_load(tier):
            if tier == SystemTier.FULL:
                return False  # Can't load FULL
            elif tier == SystemTier.DUAL:
                detector.current_tier = tier  # Mock sets current_tier on success
                return True
            return False

        with patch.object(detector, 'attempt_load_models', side_effect=mock_attempt_load):
            new_tier = detector.attempt_tier_upgrade()

            assert new_tier == SystemTier.DUAL
            assert detector.current_tier == SystemTier.DUAL

    def test_no_tier_upgrade_available(self, mock_config):
        """No tier upgrade when no higher tier loads successfully."""
        detector = TierDetector(mock_config)
        detector.current_tier = SystemTier.SINGLE

        # Mock model loading to fail for all higher tiers
        with patch.object(detector, 'attempt_load_models', return_value=False):

            new_tier = detector.attempt_tier_upgrade()

            assert new_tier is None
            assert detector.current_tier == SystemTier.SINGLE  # Unchanged

    def test_already_at_highest_tier(self, mock_config):
        """Already at highest tier returns None."""
        detector = TierDetector(mock_config)
        detector.current_tier = SystemTier.FULL

        new_tier = detector.attempt_tier_upgrade()

        assert new_tier is None
        assert detector.current_tier == SystemTier.FULL


class TestModelLoading:
    """Test model loading functionality."""

    @patch('prism_mcp.models.embedder.load_embedder_models')
    def test_successful_model_loading(self, mock_load_models, mock_config):
        """Successful model loading returns True."""
        mock_load_models.return_value = True

        detector = TierDetector(mock_config)
        success = detector.attempt_load_models(SystemTier.DUAL)

        assert success is True
        assert detector.current_tier == SystemTier.DUAL

        # Verify correct parameters passed
        mock_load_models.assert_called_once_with(
            models=["e5", "starcoder2"],
            device="cuda",
            enable_reranking=False
        )

    @patch('prism_mcp.models.embedder.load_embedder_models')
    def test_failed_model_loading(self, mock_load_models, mock_config):
        """Failed model loading returns False."""
        mock_load_models.return_value = False

        detector = TierDetector(mock_config)
        success = detector.attempt_load_models(SystemTier.DUAL)

        assert success is False

    @patch('prism_mcp.models.embedder.load_embedder_models')
    def test_oom_error_during_loading(self, mock_load_models, mock_config):
        """OOM error during loading returns False."""
        mock_load_models.side_effect = RuntimeError("CUDA out of memory")

        detector = TierDetector(mock_config)
        success = detector.attempt_load_models(SystemTier.FULL)

        assert success is False

    @patch('prism_mcp.models.embedder.load_embedder_models')
    def test_non_oom_error_reraises(self, mock_load_models, mock_config):
        """Non-OOM errors are re-raised."""
        mock_load_models.side_effect = RuntimeError("Model file not found")

        detector = TierDetector(mock_config)

        with pytest.raises(RuntimeError, match="Model file not found"):
            detector.attempt_load_models(SystemTier.DUAL)

    def test_cpu_tier_uses_cpu_device(self, mock_config):
        """CPU tier uses CPU device."""
        with patch('prism_mcp.models.embedder.load_embedder_models') as mock_load:
            mock_load.return_value = True

            detector = TierDetector(mock_config)
            detector.attempt_load_models(SystemTier.CPU)

            mock_load.assert_called_once_with(
                models=["e5"],
                device="cpu",
                enable_reranking=False
            )


class TestTierInfo:
    """Test tier information reporting."""

    def test_tier_info_before_detection(self, mock_config):
        """Tier info before detection returns not_detected."""
        detector = TierDetector(mock_config)
        info = detector.get_tier_info()

        assert info["status"] == "not_detected"

    def test_tier_info_after_detection(self, mock_config):
        """Tier info after detection returns complete information."""
        with patch('torch.cuda.is_available', return_value=True), \
             patch('torch.cuda.get_device_properties') as mock_props:

            # Mock 16GB VRAM
            mock_device = Mock()
            mock_device.total_memory = 16 * (1024**3)
            mock_props.return_value = mock_device

            detector = TierDetector(mock_config)
            detector.detect_tier()
            info = detector.get_tier_info()

            assert info["tier"] == "Dual"
            assert info["vram_available_gb"] == 16.0
            assert info["models_loaded"] == ["e5", "starcoder2"]
            assert info["features"]["semantic_search"] is True
            assert info["features"]["code_patterns"] is True
            assert info["features"]["reranking"] is False
            assert "description" in info


class TestSingletonPattern:
    """Test singleton pattern for get_tier_detector."""

    def test_singleton_requires_config_first_call(self):
        """get_tier_detector requires config on first call."""
        # Clear any existing instance
        import prism_mcp.models.tier_detector
        prism_mcp.models.tier_detector._tier_detector_instance = None

        with pytest.raises(RuntimeError, match="TierDetector config required"):
            get_tier_detector()

    def test_singleton_returns_same_instance(self, mock_config):
        """get_tier_detector returns same instance on subsequent calls."""
        # Clear any existing instance
        import prism_mcp.models.tier_detector
        prism_mcp.models.tier_detector._tier_detector_instance = None

        detector1 = get_tier_detector(mock_config)
        detector2 = get_tier_detector()  # No config needed

        assert detector1 is detector2


class TestLoadEmbedderModels:
    """Test the load_embedder_models function."""

    def test_load_e5_only(self):
        """Loading E5 only succeeds."""
        result = load_embedder_models(["e5"], "cuda", False)
        # This is a placeholder implementation, so just verify it returns True
        assert result is True

    def test_load_multiple_models(self):
        """Loading multiple models succeeds."""
        result = load_embedder_models(["e5", "starcoder2"], "cuda", False)
        assert result is True

    def test_load_with_reranking(self):
        """Loading with reranking enabled succeeds."""
        result = load_embedder_models(["e5", "starcoder2", "bge"], "cuda", True)
        assert result is True

    def test_load_on_cpu(self):
        """Loading on CPU succeeds."""
        result = load_embedder_models(["e5"], "cpu", False)
        assert result is True