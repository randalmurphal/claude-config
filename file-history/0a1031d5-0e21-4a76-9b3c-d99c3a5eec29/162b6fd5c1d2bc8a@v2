"""
Production Project Indexer - Indexes entire projects for semantic search.

Handles:
- Multiple languages (Python, JS/TS, Go, etc.)
- Documentation files (MD, RST, TXT)
- Large monorepos (streaming, batching)
- Progress reporting
- Idempotent operations
"""

import ast
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterator, Tuple
from dataclasses import dataclass, asdict
import logging

logger = logging.getLogger(__name__)

# Skip these directories
SKIP_DIRS = {
    'node_modules', '.git', '.venv', 'venv', '__pycache__',
    'build', 'dist', '.pytest_cache', '.mypy_cache', '.tox',
    'coverage', '.coverage', 'htmlcov', '.eggs', '*.egg-info',
    'target', 'bin', 'obj', '.next', '.nuxt', 'out',
    'logs', 'tmp', 'temp', 'cache', '.cache'
}

# File extensions to index
CODE_EXTENSIONS = {
    '.py': 'python',
    '.js': 'javascript',
    '.jsx': 'javascript',
    '.ts': 'typescript',
    '.tsx': 'typescript',
    '.go': 'go',
    '.rs': 'rust',
    '.java': 'java',
    '.c': 'c',
    '.cpp': 'cpp',
    '.h': 'c',
    '.hpp': 'cpp',
    '.cs': 'csharp',
    '.rb': 'ruby',
    '.php': 'php',
    '.swift': 'swift',
    '.kt': 'kotlin',
}

DOC_EXTENSIONS = {'.md', '.rst', '.txt', '.adoc', '.textile'}


@dataclass
class IndexProgress:
    """Progress report for indexing"""
    total_files: int
    processed_files: int
    indexed_memories: int
    errors: int
    current_file: Optional[str] = None
    status: str = "running"

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class ProjectIndexer:
    """
    Production-ready project indexer.

    Indexes code + docs into PRISM for semantic search.
    """

    def __init__(self, storage_manager):
        """
        Args:
            storage_manager: Instance with store_memory() method
        """
        self.storage = storage_manager
        self.progress = None

    def index_project(
        self,
        root_dir: str,
        project_id: Optional[str] = None,
        session_id: str = "project_indexing",
        include_docs: bool = True,
        include_tests: bool = False,
        max_file_size_kb: int = 500,
        batch_size: int = 10,
    ) -> IndexProgress:
        """
        Index an entire project.

        Args:
            root_dir: Root directory to index
            project_id: Optional project identifier
            session_id: Session ID for indexing
            include_docs: Index documentation files
            include_tests: Index test files
            max_file_size_kb: Skip files larger than this
            batch_size: Batch size for memory storage

        Returns:
            IndexProgress with final statistics
        """
        root_path = Path(root_dir).resolve()
        if not root_path.exists():
            raise ValueError(f"Directory does not exist: {root_dir}")

        if not project_id:
            project_id = root_path.name

        # Discover files
        files = list(self._discover_files(
            root_path,
            include_docs=include_docs,
            include_tests=include_tests,
            max_size_kb=max_file_size_kb
        ))

        self.progress = IndexProgress(
            total_files=len(files),
            processed_files=0,
            indexed_memories=0,
            errors=0
        )

        logger.info(f"Indexing {len(files)} files from {root_path}")

        # Process files
        batch = []
        for file_path, file_type in files:
            self.progress.current_file = str(file_path.relative_to(root_path))

            try:
                memories = self._index_file(
                    file_path=file_path,
                    file_type=file_type,
                    root_dir=root_path,
                    project_id=project_id,
                    session_id=session_id
                )

                batch.extend(memories)

                # Store batch
                if len(batch) >= batch_size:
                    self._store_batch(batch)
                    self.progress.indexed_memories += len(batch)
                    batch = []

            except Exception as e:
                logger.error(f"Error indexing {file_path}: {e}")
                self.progress.errors += 1

            self.progress.processed_files += 1

        # Store remaining
        if batch:
            self._store_batch(batch)
            self.progress.indexed_memories += len(batch)

        self.progress.status = "complete"
        self.progress.current_file = None

        logger.info(
            f"Indexing complete: {self.progress.indexed_memories} memories, "
            f"{self.progress.errors} errors"
        )

        return self.progress

    def _discover_files(
        self,
        root_dir: Path,
        include_docs: bool,
        include_tests: bool,
        max_size_kb: int
    ) -> Iterator[Tuple[Path, str]]:
        """
        Discover files to index.

        Yields:
            (file_path, file_type) tuples
        """
        for root, dirs, files in os.walk(root_dir):
            # Skip unwanted directories
            dirs[:] = [d for d in dirs if d not in SKIP_DIRS]

            root_path = Path(root)

            for file in files:
                file_path = root_path / file
                suffix = file_path.suffix.lower()

                # Check size
                try:
                    size_kb = file_path.stat().st_size / 1024
                    if size_kb > max_size_kb:
                        continue
                except:
                    continue

                # Skip tests if not included
                if not include_tests and ('test' in str(file_path).lower()):
                    continue

                # Code files
                if suffix in CODE_EXTENSIONS:
                    yield (file_path, CODE_EXTENSIONS[suffix])

                # Doc files
                elif include_docs and suffix in DOC_EXTENSIONS:
                    yield (file_path, 'documentation')

    def _index_file(
        self,
        file_path: Path,
        file_type: str,
        root_dir: Path,
        project_id: str,
        session_id: str
    ) -> List[Dict[str, Any]]:
        """
        Index a single file.

        Returns:
            List of memory dicts to store
        """
        rel_path = file_path.relative_to(root_dir)
        content = file_path.read_text(encoding='utf-8', errors='ignore')

        memories = []

        if file_type == 'python':
            memories.extend(self._index_python_file(
                content, rel_path, project_id, session_id
            ))
        elif file_type == 'documentation':
            memories.extend(self._index_documentation(
                content, rel_path, project_id, session_id
            ))
        else:
            # For other languages, store file-level summary
            memories.append(self._create_file_summary(
                content, rel_path, file_type, project_id, session_id
            ))

        return memories

    def _index_python_file(
        self,
        content: str,
        rel_path: Path,
        project_id: str,
        session_id: str
    ) -> List[Dict[str, Any]]:
        """Extract Python classes and functions"""
        memories = []

        try:
            tree = ast.parse(content)

            # Module docstring
            module_doc = ast.get_docstring(tree)
            if module_doc and len(module_doc) > 30:
                memories.append({
                    'content': f"Module {rel_path}: {module_doc}",
                    'memory_type': 'research_note',
                    'context': {
                        'session_id': session_id,
                        'project_id': project_id,
                        'file_path': str(rel_path),
                        'symbol_type': 'module',
                        'language': 'python'
                    },
                    'tags': ['documentation', 'python', project_id, 'module']
                })

            # Extract classes and functions
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_code = ast.get_source_segment(content, node)
                    if class_code and len(class_code) < 3000:
                        docstring = ast.get_docstring(node) or ""

                        # Code pattern
                        memories.append({
                            'content': class_code[:2000],
                            'memory_type': 'code_pattern',
                            'context': {
                                'session_id': session_id,
                                'project_id': project_id,
                                'file_path': str(rel_path),
                                'symbol_name': node.name,
                                'symbol_type': 'class',
                                'language': 'python'
                            },
                            'tags': ['python', 'class', project_id]
                        })

                        # Semantic note
                        if docstring:
                            memories.append({
                                'content': f"Class `{node.name}` in {rel_path}: {docstring[:300]}",
                                'memory_type': 'research_note',
                                'context': {
                                    'session_id': session_id,
                                    'project_id': project_id,
                                    'file_path': str(rel_path),
                                    'symbol_name': node.name,
                                    'language': 'python'
                                },
                                'tags': ['architecture', 'python', project_id]
                            })

                elif isinstance(node, ast.FunctionDef):
                    if not node.name.startswith('_') or ast.get_docstring(node):
                        func_code = ast.get_source_segment(content, node)
                        if func_code and len(func_code) < 2000:
                            memories.append({
                                'content': func_code[:1500],
                                'memory_type': 'code_pattern',
                                'context': {
                                    'session_id': session_id,
                                    'project_id': project_id,
                                    'file_path': str(rel_path),
                                    'symbol_name': node.name,
                                    'symbol_type': 'function',
                                    'language': 'python'
                                },
                                'tags': ['python', 'function', project_id]
                            })

        except SyntaxError:
            pass
        except Exception as e:
            logger.warning(f"Error parsing Python file {rel_path}: {e}")

        return memories

    def _index_documentation(
        self,
        content: str,
        rel_path: Path,
        project_id: str,
        session_id: str
    ) -> List[Dict[str, Any]]:
        """Index documentation files"""
        memories = []

        # Split into chunks (by sections for markdown)
        if rel_path.suffix == '.md':
            chunks = self._split_markdown(content)
        else:
            chunks = [content]

        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) < 50:
                continue

            memories.append({
                'content': chunk[:2000],
                'memory_type': 'research_note',
                'context': {
                    'session_id': session_id,
                    'project_id': project_id,
                    'file_path': str(rel_path),
                    'chunk_index': i
                },
                'tags': ['documentation', project_id, rel_path.stem]
            })

        return memories

    def _create_file_summary(
        self,
        content: str,
        rel_path: Path,
        file_type: str,
        project_id: str,
        session_id: str
    ) -> Dict[str, Any]:
        """Create summary for non-Python code files"""
        # Extract first comment/docstring as summary
        summary = content[:500]

        return {
            'content': f"File {rel_path} ({file_type}):\n{summary}",
            'memory_type': 'code_pattern',
            'context': {
                'session_id': session_id,
                'project_id': project_id,
                'file_path': str(rel_path),
                'language': file_type
            },
            'tags': [file_type, project_id, 'source']
        }

    def _split_markdown(self, content: str) -> List[str]:
        """Split markdown by sections"""
        chunks = []
        current = []

        for line in content.split('\n'):
            if line.startswith('#'):
                if current:
                    chunks.append('\n'.join(current))
                    current = []
            current.append(line)

        if current:
            chunks.append('\n'.join(current))

        return chunks

    def _store_batch(self, batch: List[Dict[str, Any]]):
        """Store a batch of memories"""
        for memory_data in batch:
            try:
                self.storage.store_memory(**memory_data)
            except Exception as e:
                logger.error(f"Error storing memory: {e}")
                self.progress.errors += 1