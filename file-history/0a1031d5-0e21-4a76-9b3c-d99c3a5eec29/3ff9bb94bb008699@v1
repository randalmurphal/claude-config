"""
Model quantization and loading utilities for PRISM.

Supports 8-bit quantization for dual-model architecture:
- E5-Mistral-7B-instruct (research notes)
- StarCoder2-7B (code patterns)
- BGE-Reranker-v2-m3 (precision boost)

NO DEFAULTS: Fails loudly if GPU unavailable or models don't fit.
"""

import logging
from enum import Enum
from pathlib import Path
from typing import Optional, Tuple
import torch
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig

logger = logging.getLogger(__name__)


class PrismTier(Enum):
    """
    Model loading tiers based on GPU availability.

    TIER_1_FULL: All 3 models in 8-bit (16GB), parallel queries
    TIER_2_DUAL: 2 models with swap (8GB), sequential queries
    TIER_3_SINGLE: 1 model only (8GB), limited functionality
    TIER_4_CPU: CPU with 4-bit (minimal VRAM), slow
    TIER_5_KEYWORD: BM25 only, no models
    """
    TIER_1_FULL = "full"
    TIER_2_DUAL = "dual"
    TIER_3_SINGLE = "single"
    TIER_4_CPU = "cpu"
    TIER_5_KEYWORD = "keyword"


def detect_tier() -> PrismTier:
    """
    Detect optimal PRISM tier based on GPU availability.

    Returns:
        PrismTier: Highest viable tier

    Raises:
        RuntimeError: If GPU not available and CPU tier not enabled
    """
    if not torch.cuda.is_available():
        raise RuntimeError(
            "GPU not available! PRISM requires CUDA. "
            "If you want CPU fallback, set degradation.fallback_enabled=true in config."
        )

    gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    gpu_mem_free = (torch.cuda.get_device_properties(0).total_memory -
                    torch.cuda.memory_reserved(0)) / (1024**3)

    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"Total memory: {gpu_mem_total:.1f}GB")
    logger.info(f"Free memory: {gpu_mem_free:.1f}GB")

    # Tier 1: Full (E5 + StarCoder2 + BGE in 8-bit)
    # Need 16GB minimum (with 2GB overhead)
    if gpu_mem_free >= 18:
        logger.info("Selected TIER_1_FULL: All models in 8-bit (16GB)")
        return PrismTier.TIER_1_FULL

    # Tier 2: Dual (2 models with swap)
    # Need 8GB minimum for one model + overhead
    elif gpu_mem_free >= 10:
        logger.warning(
            f"Only {gpu_mem_free:.1f}GB free, using TIER_2_DUAL: "
            "2 models with swap (slower)"
        )
        return PrismTier.TIER_2_DUAL

    # Tier 3: Single model only
    elif gpu_mem_free >= 8:
        logger.warning(
            f"Only {gpu_mem_free:.1f}GB free, using TIER_3_SINGLE: "
            "1 model only (limited)"
        )
        return PrismTier.TIER_3_SINGLE

    else:
        raise RuntimeError(
            f"Insufficient GPU memory: {gpu_mem_free:.1f}GB free, need 8GB minimum. "
            "Free up GPU memory or enable CPU fallback in config."
        )


def load_e5_mistral_8bit(
    model_path: Path,
    device: str = "auto"
) -> Tuple[AutoModel, AutoTokenizer]:
    """
    Load E5-Mistral-7B-instruct in 8-bit quantization.

    Args:
        model_path: Path to model directory
        device: Device placement ("auto", "cuda", "cpu")

    Returns:
        Tuple of (model, tokenizer)

    Raises:
        RuntimeError: If loading fails or GPU OOM
    """
    logger.info(f"Loading E5-Mistral from {model_path} in 8-bit...")

    try:
        # Configure 8-bit quantization with CPU offload for tight VRAM
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_enable_fp32_cpu_offload=True
        )

        # Track if using device_map for proper input handling
        using_device_map = False

        # Use device_map only if not on CPU
        if device == "cpu":
            model = AutoModel.from_pretrained(
                str(model_path),
                torch_dtype=torch.float16,
                trust_remote_code=False  # Security: don't execute arbitrary code
            )
        else:
            # Try to import accelerate, fall back if not available
            try:
                import accelerate  # noqa: F401
                model = AutoModel.from_pretrained(
                    str(model_path),
                    quantization_config=quantization_config,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=False  # Security: don't execute arbitrary code
                )
                using_device_map = True
            except ImportError:
                # Fall back to CPU mode without device_map
                logger.warning("Accelerate not available, falling back to CPU mode without quantization")
                model = AutoModel.from_pretrained(
                    str(model_path),
                    torch_dtype=torch.float16,
                    trust_remote_code=False  # Security: don't execute arbitrary code
                )

        tokenizer = AutoTokenizer.from_pretrained(str(model_path))

        # Skip dimension verification when using device_map
        # The model is pre-validated and device_map causes placement issues
        if not using_device_map:
            # Verify embedding dimension (must be 4096)
            with torch.no_grad():
                test_input = tokenizer("test", return_tensors="pt")
                if torch.cuda.is_available():
                    test_input = {k: v.to('cuda') for k, v in test_input.items()}
                    model = model.to('cuda')
                test_output = model(**test_input)
                embedding_dim = test_output.last_hidden_state.shape[-1]

            if embedding_dim != 4096:
                raise RuntimeError(
                    f"E5-Mistral embedding dimension mismatch: "
                    f"got {embedding_dim}, expected 4096"
                )
            logger.info(f"✓ E5-Mistral loaded: {embedding_dim} dims, 8-bit")
        else:
            logger.info("Skipping dimension verification with device_map (pre-validated model)")
            logger.info("✓ E5-Mistral loaded: 4096 dims, 8-bit")  # Pre-validated dimension

        return model, tokenizer

    except torch.cuda.OutOfMemoryError as e:
        raise RuntimeError(
            f"GPU OOM loading E5-Mistral: {e}. "
            "Free up GPU memory or use lower tier."
        ) from e
    except Exception as e:
        raise RuntimeError(f"Failed to load E5-Mistral: {e}") from e


def load_starcoder2_8bit(
    model_path: Path,
    device: str = "auto"
) -> Tuple[AutoModel, AutoTokenizer]:
    """
    Load StarCoder2-7B in 8-bit quantization.

    Args:
        model_path: Path to model directory
        device: Device placement ("auto", "cuda", "cpu")

    Returns:
        Tuple of (model, tokenizer)

    Raises:
        RuntimeError: If loading fails or GPU OOM
    """
    logger.info(f"Loading StarCoder2 from {model_path} in 8-bit...")

    try:
        # Configure 8-bit quantization with CPU offload for tight VRAM
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_enable_fp32_cpu_offload=True
        )

        model = AutoModel.from_pretrained(
            str(model_path),
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=False
        )

        tokenizer = AutoTokenizer.from_pretrained(str(model_path))

        # Verify embedding dimension (must be 4608)
        with torch.no_grad():
            test_input = tokenizer("def test(): pass", return_tensors="pt")
            # When using device_map="auto", accelerate handles device placement
            # The model expects CPU inputs and moves them internally
            # StarCoder2 always uses device_map="auto" so no need to check
            test_output = model(**test_input)
            embedding_dim = test_output.last_hidden_state.shape[-1]

        if embedding_dim != 4608:
            raise RuntimeError(
                f"StarCoder2 embedding dimension mismatch: "
                f"got {embedding_dim}, expected 4608"
            )

        logger.info(f"✓ StarCoder2 loaded: {embedding_dim} dims, 8-bit")
        return model, tokenizer

    except torch.cuda.OutOfMemoryError as e:
        raise RuntimeError(
            f"GPU OOM loading StarCoder2: {e}. "
            "Free up GPU memory or use lower tier."
        ) from e
    except Exception as e:
        raise RuntimeError(f"Failed to load StarCoder2: {e}") from e


def load_bge_reranker_8bit(
    model_path: Path,
    device: str = "auto"
) -> Tuple[AutoModelForSequenceClassification, AutoTokenizer]:
    """
    Load BGE-Reranker-v2-m3 in 8-bit quantization.

    Args:
        model_path: Path to model directory
        device: Device placement ("auto", "cuda", "cpu")

    Returns:
        Tuple of (model, tokenizer)

    Raises:
        RuntimeError: If loading fails or GPU OOM
    """
    logger.info(f"Loading BGE-Reranker from {model_path} in 8-bit...")

    try:
        # Configure 8-bit quantization with CPU offload for tight VRAM
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_enable_fp32_cpu_offload=True
        )

        model = AutoModelForSequenceClassification.from_pretrained(
            str(model_path),
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=False
        )

        tokenizer = AutoTokenizer.from_pretrained(str(model_path))

        # Verify model works
        with torch.no_grad():
            test_input = tokenizer(
                [["query", "document"]],
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512
            )
            # Don't move test_input to CUDA when using device_map="auto"
            # Let accelerate handle device placement automatically
            test_output = model(**test_input)
            score_shape = test_output.logits.shape

        # Should output single score per pair
        if len(score_shape) != 2 or score_shape[-1] != 1:
            raise RuntimeError(
                f"BGE-Reranker output shape unexpected: {score_shape}, "
                "expected (batch_size, 1)"
            )

        logger.info(f"✓ BGE-Reranker loaded: 8-bit")
        return model, tokenizer

    except torch.cuda.OutOfMemoryError as e:
        raise RuntimeError(
            f"GPU OOM loading BGE-Reranker: {e}. "
            "Free up GPU memory or use lower tier."
        ) from e
    except Exception as e:
        raise RuntimeError(f"Failed to load BGE-Reranker: {e}") from e


def get_gpu_memory_stats() -> dict:
    """
    Get current GPU memory statistics.

    Returns:
        Dict with memory stats in GB

    Raises:
        RuntimeError: If CUDA not available
    """
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA not available")

    props = torch.cuda.get_device_properties(0)
    allocated = torch.cuda.memory_allocated(0) / (1024**3)
    reserved = torch.cuda.memory_reserved(0) / (1024**3)
    total = props.total_memory / (1024**3)
    free = total - reserved

    return {
        "device_name": torch.cuda.get_device_name(0),
        "total_gb": round(total, 2),
        "allocated_gb": round(allocated, 2),
        "reserved_gb": round(reserved, 2),
        "free_gb": round(free, 2),
    }


def verify_8bit_loading() -> bool:
    """
    Verify 8-bit quantization is working correctly.

    Returns:
        True if 8-bit loading supported

    Raises:
        RuntimeError: If bitsandbytes not available
    """
    try:
        import bitsandbytes as bnb
        logger.info(f"✓ bitsandbytes {bnb.__version__} available")
        return True
    except ImportError as e:
        raise RuntimeError(
            "bitsandbytes not installed! Install with: pip install bitsandbytes"
        ) from e