#!/usr/bin/env python3
"""
Pattern Loader for PRISM - API Version
Loads processed patterns from downloaded_data into Qdrant and Neo4j via HTTP API
This version doesn't load models locally, uses the API server instead.
"""

import json
import logging
import httpx
import asyncio
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import argparse

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class PatternLoaderAPI:
    """Loads processed patterns into PRISM databases via HTTP API"""

    def __init__(self, api_url: str = "http://localhost:8090", api_key: str = "prism_development_key_2024"):
        """Initialize the pattern loader"""
        logger.info(f"Initializing Pattern Loader API client for {api_url}")

        self.api_url = api_url
        self.api_key = api_key
        self.headers = {"Authorization": f"Bearer {api_key}"}

        # Get data path from environment or default
        base_path = Path("~/.local/share/prism_mcp").expanduser()
        self.processed_dir = base_path / "processed_patterns"

        if not self.processed_dir.exists():
            raise RuntimeError(f"Processed patterns directory not found: {self.processed_dir}")

        self.stats = {
            "total_loaded": 0,
            "batches_processed": 0,
            "failed": 0,
            "start_time": datetime.now()
        }

    async def check_api_health(self) -> bool:
        """Check if API server is healthy and ready"""
        async with httpx.AsyncClient() as client:
            try:
                response = await client.get(f"{self.api_url}/health")
                if response.status_code == 200:
                    health = response.json()
                    logger.info(f"API server healthy: {health}")
                    return True
            except Exception as e:
                logger.error(f"API health check failed: {e}")
                return False
        return False

    async def generate_embeddings(self, texts: List[str], memory_type: str = "code_pattern") -> List[List[float]]:
        """Generate embeddings via API"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.api_url}/api/embeddings/batch",
                    json={
                        "texts": texts,
                        "memory_type": memory_type
                    },
                    headers=self.headers
                )
                if response.status_code == 200:
                    result = response.json()
                    return result["embeddings"]
                else:
                    logger.error(f"Embedding generation failed: {response.status_code} - {response.text}")
                    return []
            except Exception as e:
                logger.error(f"Error generating embeddings: {e}")
                return []

    async def upload_to_qdrant(self, patterns: List[Dict], collection_name: str = "code_patterns", force_recompute: bool = True):
        """Upload patterns to Qdrant via API"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.api_url}/api/patterns/bulk",
                    json={
                        "patterns": patterns,
                        "collection": collection_name,
                        "force_recompute": force_recompute,  # Force GPU computation, bypass cache
                        "create_relationships": True  # Create Neo4j nodes for graph expansion
                    },
                    headers=self.headers
                )
                if response.status_code == 200:
                    result = response.json()
                    return result.get("loaded", 0)
                else:
                    logger.error(f"Pattern upload failed: {response.status_code} - {response.text}")
                    return 0
            except Exception as e:
                logger.error(f"Error uploading patterns: {e}")
                return 0

    def _create_embedding_text(self, pattern: Dict[str, Any]) -> str:
        """Create text for embedding from pattern data"""
        parts = []

        # Add pattern name
        if pattern.get("pattern_name"):
            parts.append(f"Pattern: {pattern['pattern_name']}")

        # Add pattern type and semantic context
        if pattern.get("pattern_type"):
            parts.append(f"Type: {pattern['pattern_type']}")
        if pattern.get("semantic_context"):
            parts.append(f"Context: {pattern['semantic_context']}")

        # Add the actual pattern code and AST signature
        if pattern.get("ast_signature"):
            parts.append(f"AST Signature: {pattern['ast_signature']}")
        if pattern.get("code_snippet"):
            parts.append(f"Code: {pattern['code_snippet']}")

        # Add language context
        if pattern.get("language"):
            parts.append(f"Language: {pattern['language']}")

        # Add quality score instead of confidence
        if pattern.get("quality_score"):
            parts.append(f"Quality: {pattern['quality_score']}")

        return "\n".join(parts)

    async def load_patterns(self, pattern_file: Path = None, batch_size: int = 100, start_batch: int = 25):
        """Load patterns from file into databases via API"""

        # Check API health first
        if not await self.check_api_health():
            logger.error("API server not healthy, please start it first")
            return 0

        # Default pattern file
        if pattern_file is None:
            pattern_file = self.processed_dir / "patterns.json"

        if not pattern_file.exists():
            logger.error(f"Pattern file not found: {pattern_file}")
            return 0

        # Load patterns
        logger.info(f"Loading patterns from {pattern_file}")
        with open(pattern_file, 'r') as f:
            patterns = json.load(f)

        total_patterns = len(patterns)
        logger.info(f"Found {total_patterns} patterns to load")

        # Skip already loaded batches
        if start_batch > 1:
            skip_count = (start_batch - 1) * batch_size
            patterns = patterns[skip_count:]
            logger.info(f"Skipping first {skip_count} patterns (batches 1-{start_batch-1})")
            self.stats["total_loaded"] = skip_count
            self.stats["batches_processed"] = start_batch - 1

        # Process in batches
        total_batches = (len(patterns) + batch_size - 1) // batch_size

        for i in range(0, len(patterns), batch_size):
            batch = patterns[i:i+batch_size]
            batch_num = self.stats["batches_processed"] + 1

            logger.info(f"Processing batch {batch_num}/{total_batches + start_batch - 1} ({len(batch)} patterns)...")

            # Create embedding texts
            texts = [self._create_embedding_text(p) for p in batch]

            # Generate embeddings via API
            logger.info(f"  Generating embeddings for {len(texts)} patterns...")
            embeddings = await self.generate_embeddings(texts)

            if not embeddings:
                logger.error(f"  Failed to generate embeddings for batch {batch_num}")
                self.stats["failed"] += len(batch)
                continue

            # Add embeddings to patterns
            for pattern, embedding in zip(batch, embeddings):
                pattern["embedding"] = embedding

            # Upload to databases
            logger.info(f"  Uploading batch {batch_num} to databases...")
            loaded = await self.upload_to_qdrant(batch)

            self.stats["total_loaded"] += loaded
            self.stats["batches_processed"] += 1

            # Progress report
            elapsed = (datetime.now() - self.stats["start_time"]).total_seconds()
            rate = self.stats["total_loaded"] / elapsed if elapsed > 0 else 0
            eta_seconds = (total_patterns - self.stats["total_loaded"]) / rate if rate > 0 else 0
            eta_minutes = eta_seconds / 60

            logger.info(f"  Progress: {self.stats['total_loaded']}/{total_patterns} patterns "
                       f"({self.stats['total_loaded']*100/total_patterns:.1f}%) "
                       f"Rate: {rate:.1f} patterns/sec, ETA: {eta_minutes:.1f} minutes")

        # Final stats
        elapsed = (datetime.now() - self.stats["start_time"]).total_seconds()
        logger.info("=" * 60)
        logger.info(f"PATTERN LOADING COMPLETE")
        logger.info(f"Total patterns loaded: {self.stats['total_loaded']}")
        logger.info(f"Total batches: {self.stats['batches_processed']}")
        logger.info(f"Failed: {self.stats['failed']}")
        logger.info(f"Time taken: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)")
        logger.info(f"Average rate: {self.stats['total_loaded']/elapsed:.1f} patterns/sec")
        logger.info("=" * 60)

        return self.stats["total_loaded"]


async def main():
    parser = argparse.ArgumentParser(description="Load patterns into PRISM via HTTP API")
    parser.add_argument("--api-url", default="http://localhost:8090", help="API server URL")
    parser.add_argument("--api-key", default="prism_development_key_2024", help="API key")
    parser.add_argument("--file", type=Path, help="Pattern file to load")
    parser.add_argument("--batch-size", type=int, default=100, help="Batch size for processing")
    parser.add_argument("--start-batch", type=int, default=25, help="Start from batch number (for resuming)")

    args = parser.parse_args()

    loader = PatternLoaderAPI(api_url=args.api_url, api_key=args.api_key)
    loaded = await loader.load_patterns(
        pattern_file=args.file,
        batch_size=args.batch_size,
        start_batch=args.start_batch
    )

    return 0 if loaded > 0 else 1


if __name__ == "__main__":
    exit(asyncio.run(main()))