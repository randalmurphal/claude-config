"""
Dual-model embedding system for PRISM Phase 1.

Supports:
- E5-Mistral-7B-instruct (research notes, decisions, gotchas, guidance)
- StarCoder2-7B (code patterns, extracted patterns, AST patterns)
- Voyage API (Tier 0): voyage-3-large + voyage-code-3

Both local models loaded in 8-bit quantization simultaneously.
API models used in Tier 0 (zero GPU).

NO DEFAULTS PHILOSOPHY:
- Model path must be explicitly configured
- Device must be explicitly specified
- No fallback models or dimensions
- Fail immediately if model not available
"""

import logging
import os
import time
from pathlib import Path
from typing import Any

import numpy as np
import torch
from transformers import AutoModel, AutoTokenizer

from prism_mcp.models.quantization import (
    load_bge_reranker_8bit,
    load_e5_mistral_8bit,
    load_starcoder2_8bit,
)
from prism_mcp.models.tier_detector import (
    SystemTier,
    get_tier_detector,
)
from prism_mcp.storage.qdrant_manager import (
    COLLECTION_E5,
    COLLECTION_STARCODER,
    E5_DIMENSION,
    MEMORY_TYPE_TO_COLLECTION,
    STARCODER_DIMENSION,
)
from prism_mcp.utils.config import get_config

logger = logging.getLogger(__name__)


class DualModelEmbedder:
    """
    Tri-model embedding and reranking system with 8-bit quantization.

    Models:
    - E5-Mistral-7B-instruct: Research notes (4096 dims)
    - StarCoder2-7B: Code patterns (4608 dims)
    - BGE-Reranker-v2-m3: Cross-encoder for precision boost (Phase 2.2)

    All models loaded simultaneously in 8-bit (~15GB total).
    """

    # Query augmentation negations map (Phase 2.1)
    # Learned from test_query_augmentation.py results (+33% near-miss precision)
    NEGATIONS_MAP = {
        "authorization": "NOT authentication, NOT login tokens, NOT user verification",
        "authentication": "NOT authorization, NOT permissions, NOT access control",
        "sql injection": "NOT XSS, NOT CSRF, NOT general security, NOT input validation",
        "xss": "NOT SQL injection, NOT CSRF, NOT general security",
        "csrf": "NOT XSS, NOT SQL injection, NOT general security",
        "error handling": "NOT error messages, NOT status codes, NOT logging",
        "caching": "NOT memoization, NOT browser cache, NOT CDN",
        "async": "NOT threading, NOT multiprocessing, NOT parallelism",
        "threading": "NOT async, NOT multiprocessing",
        "multiprocessing": "NOT async, NOT threading",
        "create": "NOT read, NOT select, NOT get, NOT retrieve",
        "insert": "NOT update, NOT delete, NOT select",
        "update": "NOT create, NOT insert, NOT select",
        "delete": "NOT create, NOT insert, NOT update",
        "get": "NOT post, NOT put, NOT delete",
        "post": "NOT get, NOT put, NOT delete",
        "put": "NOT get, NOT post, NOT delete",
    }

    def __init__(self):
        """
        Initialize tier-aware embedder from configuration.

        Raises:
            RuntimeError: If tier detection fails or configuration invalid
        """
        config = get_config()
        self.config = config

        # Initialize tier detector
        self.tier_detector = get_tier_detector(config)
        self.current_tier = self.tier_detector.detect_tier()

        logger.info(f"Detected system tier: {self.current_tier.value}")

        # Validate tier configuration
        if self.current_tier == SystemTier.KEYWORD:
            logger.warning(
                "Operating in KEYWORD-only mode (BM25). "
                "Semantic search and pattern detection disabled."
            )

        # Model paths (tier-aware)
        base_path = Path(config.data.base_path).expanduser()
        models_dir = config.data.models_dir

        # Check if models_dir is absolute (for containers)
        if Path(models_dir).is_absolute():
            models_base = Path(models_dir)
        else:
            models_base = base_path / models_dir

        self.e5_model_path = models_base / "e5-mistral-7b-instruct"
        self.starcoder_model_path = models_base / "starcoder2-7b"
        self.bge_model_path = models_base / "bge-reranker-v2-m3"

        # Get tier requirements
        tier_requirements = self.tier_detector.TIER_REQUIREMENTS[self.current_tier]

        # Validate required models exist based on tier
        if "e5" in tier_requirements.models:
            if not self.e5_model_path.exists():
                raise RuntimeError(
                    f"E5-Mistral required for {self.current_tier.value} tier but not found at {self.e5_model_path}. "
                    "Run: python -m prism_mcp.bootstrap.download_models"
                )

        if "starcoder2" in tier_requirements.models:
            if not self.starcoder_model_path.exists():
                raise RuntimeError(
                    f"StarCoder2 required for {self.current_tier.value} tier but not found at {self.starcoder_model_path}. "
                    "Run: python -m prism_mcp.bootstrap.download_models"
                )

        # BGE availability check (only needed for FULL tier)
        self.bge_available = self.bge_model_path.exists() and "bge" in tier_requirements.models
        if "bge" in tier_requirements.models and not self.bge_model_path.exists():
            logger.warning(
                f"BGE-Reranker required for {self.current_tier.value} tier but not found at {self.bge_model_path}. "
                "Reranking will be disabled. "
                "Run: python3 tmp_model_tests/scripts/download_bge_reranker.py"
            )
            self.bge_available = False

        # Device selection based on tier
        if self.current_tier == SystemTier.CPU:
            self.device = "cpu"
        else:
            self.device = "auto"  # Let transformers handle GPU device placement

        self.max_sequence_length = config.model.max_sequence_length

        # E5 instruction prefix (required per E5 documentation)
        self.e5_instruction = (
            "Instruct: Retrieve semantic similar text to the query: "
        )

        # Models (loaded on warm_up)
        self.e5_model: AutoModel | None = None
        self.e5_tokenizer: AutoTokenizer | None = None
        self.starcoder_model: AutoModel | None = None
        self.starcoder_tokenizer: AutoTokenizer | None = None
        self.bge_model: Any | None = (
            None  # AutoModelForSequenceClassification
        )
        self.bge_tokenizer: AutoTokenizer | None = None
        self._models_loaded = False

        # Simple embedding cache
        self._embedding_cache: dict[str, np.ndarray] = {}

        # Phase 3.1: Learned negations support
        self.enable_learned_negations = (
            config.query_augmentation.enable_learned_negations
        )
        self.reload_interval_queries = (
            config.query_augmentation.reload_interval_queries
        )
        self.min_confidence = config.query_augmentation.min_confidence
        self.decay_half_life_days = (
            config.query_augmentation.negation_decay_half_life_days
        )  # Phase 3.4
        self._learned_negations_map: dict[str, str] = {}
        self._query_count = 0

        # Load learned negations if enabled
        if self.enable_learned_negations:
            try:
                self._load_learned_negations()
                logger.info(
                    f"Loaded {len(self._learned_negations_map)} learned negations from Neo4j"
                )
            except Exception as e:
                logger.warning(f"Failed to load learned negations: {e}")
                logger.warning("Continuing with hardcoded negations only")

        logger.info(f"TierAwareEmbedder initialized (tier={self.current_tier.value})")
        logger.info(f"Tier requirements: {tier_requirements.description}")
        logger.info(f"Models to load: {tier_requirements.models}")
        logger.info(f"Features enabled: {tier_requirements.features}")

        if "e5" in tier_requirements.models:
            logger.info(f"E5 path: {self.e5_model_path}")
        if "starcoder2" in tier_requirements.models:
            logger.info(f"StarCoder2 path: {self.starcoder_model_path}")
        if self.bge_available:
            logger.info(f"BGE path: {self.bge_model_path}")

        # Tier 0 (PREMIUM_API): Initialize Voyage API client
        self.voyage_client = None
        if self.current_tier == SystemTier.PREMIUM_API:
            try:
                import voyageai

                # Try to load API key from .env file first, then environment
                voyage_key = None

                # Find .env file using proper package path
                import prism_mcp
                package_root = Path(prism_mcp.__file__).parent.parent
                env_file = package_root / "tests" / "model_tests" / "api_comparison" / ".env"

                if env_file.exists():
                    # Parse .env file manually (avoid extra dependency)
                    for line in env_file.read_text().splitlines():
                        line = line.strip()
                        if line.startswith("VOYAGE_API_KEY="):
                            voyage_key = line.split("=", 1)[1].strip()
                            logger.info(f"✓ Loaded VOYAGE_API_KEY from {env_file}")
                            break

                # Fallback to environment variable
                if not voyage_key:
                    voyage_key = os.getenv("VOYAGE_API_KEY")

                if not voyage_key:
                    raise RuntimeError(
                        "VOYAGE_API_KEY required for Tier 0 (PREMIUM_API).\n"
                        "Either:\n"
                        "  1. Set in tests/model_tests/api_comparison/.env file (VOYAGE_API_KEY=your-key)\n"
                        "  2. Set environment variable: export VOYAGE_API_KEY='your-key'\n"
                        "Get free API key (200M tokens): https://www.voyageai.com/"
                    )

                self.voyage_client = voyageai.Client(api_key=voyage_key)
                logger.info("✓ Voyage API client initialized (voyage-3-large + voyage-code-3)")
                logger.info("  Zero GPU memory required, SOTA embedding quality")
            except ImportError:
                raise RuntimeError(
                    "voyageai package required for Tier 0.\n"
                    "Install with: pip install voyageai"
                )

    def _load_learned_negations(self):
        """
        Load learned negations from Neo4j.

        Phase 3.1: Loads LearnedNegation nodes with confidence >= min_confidence.
        Phase 3.4: Applies age-based confidence decay.

        Raises:
            RuntimeError: If Neo4j unavailable (fail loud)
        """
        try:
            from prism_mcp.core.negation_learner import get_negation_learner

            learner = get_negation_learner()
            # Phase 3.4: Pass decay parameters
            all_learned = learner.get_learned_negations(
                decay_half_life_days=self.decay_half_life_days,
                min_confidence=self.min_confidence,
            )

            self._learned_negations_map = all_learned

            logger.debug(
                f"Loaded learned negations: {list(all_learned.keys())}"
            )

        except Exception as e:
            raise RuntimeError(
                f"Failed to load learned negations from Neo4j: {e}"
            ) from e

    def _reload_learned_negations_if_needed(self):
        """
        Reload learned negations if reload interval reached.

        Phase 3.1: Periodically reload to pick up new negations without restart.
        """
        if not self.enable_learned_negations:
            return

        if self.reload_interval_queries == 0:
            return  # Never reload

        self._query_count += 1

        if self._query_count >= self.reload_interval_queries:
            try:
                old_count = len(self._learned_negations_map)
                self._load_learned_negations()
                new_count = len(self._learned_negations_map)

                if new_count != old_count:
                    logger.info(
                        f"Reloaded learned negations: {old_count} → {new_count} "
                        f"({new_count - old_count:+d} change)"
                    )

                self._query_count = 0

            except Exception as e:
                logger.warning(f"Failed to reload learned negations: {e}")

    def _augment_query(self, query: str) -> str:
        """
        Augment query with negations to improve precision.

        Phase 2.1: Hardcoded negations (+33% near-miss precision from testing).
        Phase 3.1: Merged with learned negations (priority: learned > hardcoded).
        Phase 3.3: Multi-keyword augmentation (handles queries with multiple concepts).

        Args:
            query: Original query text

        Returns:
            Augmented query with negations if keywords detected, otherwise original query

        Examples:
            "user authorization" → "user authorization, NOT authentication, NOT login tokens"
            "authorization and authentication flow" → "query, NOT permissions, NOT tokens, NOT login"
        """
        # Reload learned negations if needed
        self._reload_learned_negations_if_needed()

        query_lower = query.lower()

        # Phase 3.3: Collect ALL matching keywords and their negations
        all_negations_set = set()  # Use set for deduplication
        matched_keywords = []

        # Check learned negations first (higher priority from feedback)
        if self.enable_learned_negations:
            for keyword, negations_str in self._learned_negations_map.items():
                if keyword in query_lower:
                    matched_keywords.append((keyword, "learned"))
                    # Parse negations (format: "NOT X, NOT Y, NOT Z")
                    negations_list = [
                        neg.strip().replace("NOT ", "").strip()
                        for neg in negations_str.split(",")
                        if neg.strip().startswith("NOT")
                    ]
                    all_negations_set.update(negations_list)

        # Also check hardcoded negations (lower priority but still include)
        for keyword, negations_str in self.NEGATIONS_MAP.items():
            if keyword in query_lower:
                # Skip if already matched in learned (learned takes priority)
                if any(kw == keyword for kw, source in matched_keywords):
                    continue

                matched_keywords.append((keyword, "hardcoded"))
                # Parse negations
                negations_list = [
                    neg.strip().replace("NOT ", "").strip()
                    for neg in negations_str.split(",")
                    if neg.strip().startswith("NOT")
                ]
                all_negations_set.update(negations_list)

        # If no keywords matched, return original query
        if not matched_keywords:
            return query

        # Phase 3.3: Combine all unique negations
        combined_negations = ", ".join(
            [f"NOT {neg}" for neg in sorted(all_negations_set)]
        )
        augmented = f"{query}, {combined_negations}"

        # Log with detail about sources
        sources = ", ".join([f"{kw}({src})" for kw, src in matched_keywords])
        logger.debug(
            f"Augmented query (multi-keyword): '{query}' → '{augmented}' "
            f"[matched: {sources}]"
        )

        return augmented

    def warm_up(self) -> dict[str, float]:
        """
        Load models based on current tier with OOM handling.

        Returns:
            Dict with load times for each model

        Raises:
            RuntimeError: If loading fails after tier downgrade
        """
        if self._models_loaded:
            logger.info("Models already loaded")
            return {"e5": 0.0, "starcoder": 0.0, "bge": 0.0, "total": 0.0}

        # Attempt to load models for current tier with OOM handling
        max_retry_attempts = 3
        for attempt in range(max_retry_attempts):
            try:
                return self._attempt_model_loading()
            except RuntimeError as e:
                if "out of memory" in str(e).lower() and attempt < max_retry_attempts - 1:
                    logger.warning(f"OOM error on attempt {attempt + 1}, attempting tier downgrade")
                    # Downgrade tier and retry
                    self.current_tier = self.tier_detector.handle_oom_error()
                    logger.info(f"Downgraded to tier: {self.current_tier.value}")
                    continue
                else:
                    raise

        raise RuntimeError("Failed to load models even after tier downgrade")

    def _attempt_model_loading(self) -> dict[str, float]:
        """
        Attempt to load models for current tier.

        Returns:
            Dict with load times for each model

        Raises:
            RuntimeError: If OOM or other critical error
        """
        tier_requirements = self.tier_detector.TIER_REQUIREMENTS[self.current_tier]
        models_to_load = tier_requirements.models

        # Handle KEYWORD tier (no models to load)
        if self.current_tier == SystemTier.KEYWORD:
            logger.info("KEYWORD tier - no models to load, using BM25 only")
            self._models_loaded = True
            return {"total": 0.0}

        num_models = len(models_to_load)
        logger.info(f"Loading {num_models} models for {self.current_tier.value} tier...")
        total_start = time.time()

        # Initialize timing dict
        load_times = {"e5": 0.0, "starcoder": 0.0, "bge": 0.0}
        model_count = 0

        # Load E5-Mistral (if required by tier)
        if "e5" in models_to_load:
            model_count += 1
            logger.info(f"[{model_count}/{num_models}] Loading E5-Mistral-7B...")
            e5_start = time.time()
            self.e5_model, self.e5_tokenizer = load_e5_mistral_8bit(
                model_path=self.e5_model_path, device=self.device
            )
            self.e5_model.eval()
            load_times["e5"] = time.time() - e5_start
            logger.info(f'✓ E5-Mistral loaded in {load_times["e5"]:.1f}s')

        # Load StarCoder2 (if required by tier)
        if "starcoder2" in models_to_load:
            model_count += 1
            logger.info(f"[{model_count}/{num_models}] Loading StarCoder2-7B...")
            sc_start = time.time()
            self.starcoder_model, self.starcoder_tokenizer = load_starcoder2_8bit(
                model_path=self.starcoder_model_path, device=self.device
            )
            # Set pad_token if not present
            if self.starcoder_tokenizer.pad_token is None:
                self.starcoder_tokenizer.pad_token = (
                    self.starcoder_tokenizer.eos_token
                )
            self.starcoder_model.eval()
            load_times["starcoder"] = time.time() - sc_start
            logger.info(f'✓ StarCoder2 loaded in {load_times["starcoder"]:.1f}s')

        # Load BGE Reranker (if required by tier and available)
        # DEBUG: Force output with print() to bypass logger config issues
        print("\n===== BGE LOADING DEBUG =====", flush=True)
        print(f"Current tier: {self.current_tier.value}", flush=True)
        print(f"Models to load: {models_to_load}", flush=True)
        print(f"BGE in models_to_load: {'bge' in models_to_load}", flush=True)
        print(f"BGE path: {self.bge_model_path}", flush=True)
        print(f"BGE path exists: {self.bge_model_path.exists()}", flush=True)
        print(f"bge_available flag: {self.bge_available}", flush=True)
        print("=============================\n", flush=True)

        if "bge" in models_to_load and self.bge_available:
            model_count += 1
            logger.info(f"[{model_count}/{num_models}] Loading BGE-Reranker-v2-m3 from {self.bge_model_path}...")
            print(f"[{model_count}/{num_models}] Loading BGE-Reranker-v2-m3...", flush=True)
            bge_start = time.time()
            try:
                self.bge_model, self.bge_tokenizer = load_bge_reranker_8bit(
                    model_path=self.bge_model_path, device=self.device
                )
                self.bge_model.eval()
                load_times["bge"] = time.time() - bge_start
                logger.info(f'✓ BGE-Reranker loaded in {load_times["bge"]:.1f}s')
                print(f"✓ BGE-Reranker loaded successfully in {load_times['bge']:.1f}s", flush=True)
            except Exception as e:
                logger.error(f"FAILED to load BGE-Reranker: {e}")
                print(f"ERROR loading BGE: {e}", flush=True)
                raise
        else:
            if "bge" in models_to_load:
                msg = f"BGE-Reranker required by tier but not available (bge_available={self.bge_available})"
                logger.warning(msg)
                print(f"WARNING: {msg}", flush=True)
            else:
                msg = f"BGE-Reranker not required by tier (models_to_load={models_to_load})"
                logger.info(msg)
                print(f"INFO: {msg}", flush=True)

        total_time = time.time() - total_start

        # Verify GPU memory usage (only if using GPU)
        if torch.cuda.is_available() and self.current_tier != SystemTier.CPU:
            mem_allocated = torch.cuda.memory_allocated(0) / (1024**3)
            mem_reserved = torch.cuda.memory_reserved(0) / (1024**3)
            logger.info(
                f"GPU memory: {mem_allocated:.2f}GB allocated, {mem_reserved:.2f}GB reserved"
            )

            # Tier-specific memory warnings
            tier_requirements = self.tier_detector.TIER_REQUIREMENTS[self.current_tier]
            if mem_reserved > tier_requirements.min_vram_gb * 1.2:  # 20% overhead tolerance
                logger.warning(
                    f"GPU memory usage {mem_reserved:.2f}GB exceeds tier expectation "
                    f"{tier_requirements.min_vram_gb}GB. May cause instability."
                )

        self._models_loaded = True

        # Build description of loaded models
        loaded_models = [model for model in models_to_load if getattr(self, f"{model}_model", None) is not None]
        models_desc = f'{self.current_tier.value} tier: {", ".join(loaded_models)}' if loaded_models else f"{self.current_tier.value} tier (no models)"

        logger.info(f"✓ {models_desc} loaded in {total_time:.1f}s total")

        # Add total time and return
        load_times["total"] = total_time
        return load_times

    def embed_with_e5(self, text: str, augment: bool = False) -> np.ndarray:
        """
        Generate semantic embedding (E5-Mistral or Voyage voyage-3-large).

        Used for: research_note, decision, gotcha, guidance, corrections, memories

        Args:
            text: Text to embed
            augment: Whether to augment query with negations (Phase 2.1: +33% precision)

        Returns:
            Embedding vector of shape (4096,) for E5 or (1024,) for Voyage

        Raises:
            RuntimeError: If embedding not available in current tier or models not loaded
        """
        # Tier 0: Use Voyage API
        if self.current_tier == SystemTier.PREMIUM_API:
            if not self.voyage_client:
                raise RuntimeError("Voyage client not initialized for Tier 0")

            # Apply query augmentation if requested
            query_text = self._augment_query(text) if augment else text

            # Cache key
            cache_key = f"voyage:general:aug={augment}:{query_text}"
            if cache_key in self._embedding_cache:
                return self._embedding_cache[cache_key]

            # Call Voyage API
            try:
                result = self.voyage_client.embed(
                    texts=[query_text],
                    model="voyage-3-large",  # General/semantic model
                    input_type="document"
                )
                embedding = np.array(result.embeddings[0], dtype=np.float32)

                # Normalize (Voyage returns normalized, but be explicit)
                embedding = embedding / (np.linalg.norm(embedding) + 1e-8)

                # Verify dimension (1024 for Voyage)
                if embedding.shape[0] != 1024:
                    raise RuntimeError(
                        f"Voyage embedding dimension mismatch: got {embedding.shape[0]}, expected 1024"
                    )

                # Cache and return
                self._embedding_cache[cache_key] = embedding
                self._limit_cache_size()
                return embedding

            except Exception as e:
                raise RuntimeError(f"Voyage API call failed: {e}") from e

        # Tier 1+: Use local E5 model
        tier_requirements = self.tier_detector.TIER_REQUIREMENTS[self.current_tier]
        if "e5" not in tier_requirements.models:
            raise RuntimeError(
                f"E5 embedding not available in {self.current_tier.value} tier. "
                f"Use Tier 0 (PREMIUM_API) or Tier 1 (FULL)."
            )

        if not self._models_loaded:
            self.warm_up()

        if self.e5_model is None:
            raise RuntimeError("E5 model not loaded despite being required by tier")

        # Apply query augmentation if requested (Phase 2.1)
        if augment:
            text = self._augment_query(text)

        # Add instruction prefix (required by E5)
        query_text = f"{self.e5_instruction}{text}"
        cache_key = f"e5:aug={augment}:{query_text}"

        # Check cache
        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]

        # Generate embedding
        with torch.no_grad():
            inputs = self.e5_tokenizer(
                query_text,
                return_tensors="pt",
                truncation=True,
                max_length=self.max_sequence_length,
                padding=True,
            )
            # Move inputs to model device (required for device_map="auto")
            inputs = {k: v.to(self.e5_model.device) for k, v in inputs.items()}

            outputs = self.e5_model(**inputs)
            hidden_states = outputs.last_hidden_state

            # Mean pooling
            attention_mask = inputs["attention_mask"]
            mask_expanded = (
                attention_mask.unsqueeze(-1)
                .expand(hidden_states.size())
                .float()
            )
            sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)
            sum_mask = torch.clamp(torch.sum(mask_expanded, dim=1), min=1e-9)
            embedding = sum_hidden / sum_mask

            # Normalize
            embedding = embedding / (
                torch.norm(embedding, dim=1, keepdim=True) + 1e-8
            )

            # Convert to numpy
            result = embedding.cpu().numpy().squeeze()

        # Verify dimension
        if result.shape[0] != E5_DIMENSION:
            raise RuntimeError(
                f"E5 embedding dimension mismatch: got {result.shape[0]}, expected {E5_DIMENSION}"
            )

        # Cache result
        self._embedding_cache[cache_key] = result
        self._limit_cache_size()

        return result

    def embed_with_starcoder(
        self, code: str, augment: bool = False
    ) -> np.ndarray:
        """
        Generate code embedding (StarCoder2 or Voyage voyage-code-3).

        Used for: code_pattern, extracted_pattern, ast_pattern

        Args:
            code: Code snippet to embed
            augment: MUST be False (reserved for future code-only context)

        Returns:
            Embedding vector of shape (4608,) for StarCoder2 or (1024,) for Voyage

        Raises:
            RuntimeError: If code embedding not available in current tier, models not loaded, or augment=True
        """
        # Tier 0: Use Voyage API
        if self.current_tier == SystemTier.PREMIUM_API:
            if not self.voyage_client:
                raise RuntimeError("Voyage client not initialized for Tier 0")

            if augment:
                raise RuntimeError("Code augmentation not supported in Tier 0")

            # Cache key
            cache_key = f"voyage:code:{code}"
            if cache_key in self._embedding_cache:
                return self._embedding_cache[cache_key]

            # Call Voyage API with code-specific model
            try:
                result = self.voyage_client.embed(
                    texts=[code],
                    model="voyage-code-3",  # Code-optimized model
                    input_type="document"
                )
                embedding = np.array(result.embeddings[0], dtype=np.float32)

                # Normalize
                embedding = embedding / (np.linalg.norm(embedding) + 1e-8)

                # Verify dimension (1024 for Voyage)
                if embedding.shape[0] != 1024:
                    raise RuntimeError(
                        f"Voyage code embedding dimension mismatch: got {embedding.shape[0]}, expected 1024"
                    )

                # Cache and return
                self._embedding_cache[cache_key] = embedding
                self._limit_cache_size()
                return embedding

            except Exception as e:
                raise RuntimeError(f"Voyage API call failed for code: {e}") from e

        # Tier 1+: Use local StarCoder2 model
        tier_requirements = self.tier_detector.TIER_REQUIREMENTS[self.current_tier]
        if "starcoder2" not in tier_requirements.models:
            raise RuntimeError(
                f"StarCoder2 embedding not available in {self.current_tier.value} tier. "
                f"Use Tier 0 (PREMIUM_API) or Tier 1 (FULL)."
            )

        if not self._models_loaded:
            self.warm_up()

        if self.starcoder_model is None:
            raise RuntimeError("StarCoder2 model not loaded despite being required by tier")

        if augment:
            raise RuntimeError(
                "StarCoder2 augmentation not supported in Phase 1. "
                "Natural language augmentation breaks code pattern accuracy."
            )

        cache_key = f"starcoder:{code}"

        # Check cache
        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]

        # Generate embedding
        with torch.no_grad():
            inputs = self.starcoder_tokenizer(
                code,
                return_tensors="pt",
                truncation=True,
                max_length=self.max_sequence_length,
                padding=True,
            )
            # Move inputs to model device (required for device_map="auto")
            inputs = {
                k: v.to(self.starcoder_model.device) for k, v in inputs.items()
            }

            outputs = self.starcoder_model(**inputs)
            hidden_states = outputs.last_hidden_state

            # Mean pooling
            attention_mask = inputs["attention_mask"]
            mask_expanded = (
                attention_mask.unsqueeze(-1)
                .expand(hidden_states.size())
                .float()
            )
            sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)
            sum_mask = torch.clamp(torch.sum(mask_expanded, dim=1), min=1e-9)
            embedding = sum_hidden / sum_mask

            # Normalize
            embedding = embedding / (
                torch.norm(embedding, dim=1, keepdim=True) + 1e-8
            )

            # Convert to numpy
            result = embedding.cpu().numpy().squeeze()

        # Verify dimension
        if result.shape[0] != STARCODER_DIMENSION:
            raise RuntimeError(
                f"StarCoder2 embedding dimension mismatch: got {result.shape[0]}, expected {STARCODER_DIMENSION}"
            )

        # Cache result
        self._embedding_cache[cache_key] = result
        self._limit_cache_size()

        return result

    def generate_embedding(self, text: str, memory_type: str) -> np.ndarray:
        """
        Generate embedding using appropriate model based on memory_type.

        Args:
            text: Text or code to embed
            memory_type: Type of memory (routes to correct model)

        Returns:
            Embedding vector (4096 for E5, 4608 for StarCoder2)

        Raises:
            RuntimeError: If memory_type unknown
        """
        # Determine collection from memory_type
        collection = MEMORY_TYPE_TO_COLLECTION.get(memory_type)
        if not collection:
            raise RuntimeError(
                f"Unknown memory_type '{memory_type}'. "
                f"Valid types: {list(MEMORY_TYPE_TO_COLLECTION.keys())}"
            )

        # Route to appropriate model
        if collection == COLLECTION_E5:
            return self.embed_with_e5(text)
        elif collection == COLLECTION_STARCODER:
            return self.embed_with_starcoder(text)
        else:
            raise RuntimeError(f"Unknown collection: {collection}")

    def _limit_cache_size(self, max_size: int = 1000):
        """Limit cache size to prevent memory issues."""
        if len(self._embedding_cache) > max_size:
            oldest_key = next(iter(self._embedding_cache))
            del self._embedding_cache[oldest_key]

    def generate_batch_embeddings(
        self, texts: list[str], memory_type: str, batch_size: int = 32
    ) -> list[np.ndarray]:
        """
        Generate embeddings for multiple texts efficiently.

        Args:
            texts: List of text strings to embed
            memory_type: Type of memory (routes to correct model)
            batch_size: Number of texts to process at once

        Returns:
            List of embedding arrays

        Raises:
            RuntimeError: If memory_type unknown
        """
        # Route to appropriate model
        collection = MEMORY_TYPE_TO_COLLECTION.get(memory_type)
        if not collection:
            raise RuntimeError(
                f"Unknown memory_type '{memory_type}'. "
                f"Valid types: {list(MEMORY_TYPE_TO_COLLECTION.keys())}"
            )

        embeddings = []
        for text in texts:
            if collection == COLLECTION_E5:
                embeddings.append(self.embed_with_e5(text))
            elif collection == COLLECTION_STARCODER:
                embeddings.append(self.embed_with_starcoder(text))
            else:
                raise RuntimeError(f"Unknown collection: {collection}")

        return embeddings

    def embed_documents(self, texts: list[str]) -> list[np.ndarray]:
        """
        Embed multiple code documents for duplication detection.

        Uses code-optimized embeddings (voyage-code-3 for Tier 0, StarCoder2 for Tier 1+).

        Args:
            texts: List of code strings to embed

        Returns:
            List of embedding arrays
        """
        # Tier 0: Use Voyage code API with batch support
        if self.current_tier == SystemTier.PREMIUM_API:
            if not self.voyage_client:
                raise RuntimeError("Voyage client not initialized for Tier 0")

            try:
                result = self.voyage_client.embed(
                    texts=texts,
                    model="voyage-code-3",
                    input_type="document"
                )
                embeddings = [np.array(emb) for emb in result.embeddings]
                logger.debug(f"Generated {len(embeddings)} code embeddings with Voyage (batch)")
                return embeddings
            except Exception as e:
                raise RuntimeError(f"Voyage code batch embedding failed: {e}") from e

        # Tier 1+: Use StarCoder2 (loop over texts)
        embeddings = []
        for text in texts:
            embeddings.append(self.embed_with_starcoder(text))

        logger.debug(f"Generated {len(embeddings)} code embeddings with StarCoder2")
        return embeddings

    def compute_similarity(
        self, embedding1: np.ndarray, embedding2: np.ndarray
    ) -> float:
        """
        Compute cosine similarity between two embeddings.

        Args:
            embedding1: First embedding vector
            embedding2: Second embedding vector

        Returns:
            Cosine similarity score between -1 and 1
        """
        # Compute dot product (embeddings are already normalized)
        return float(np.dot(embedding1, embedding2))

    def clear_cache(self):
        """Clear the embedding cache."""
        self._embedding_cache.clear()
        logger.info("Embedding cache cleared")

    def rerank_with_bge(
        self, query: str, candidates: list[str], top_k: int | None = None
    ) -> list[tuple[int, float]]:
        """
        Rerank candidates using BGE cross-encoder.

        Phase 2.2: BGE provides +33-50% near-miss precision on complex scenarios.

        Args:
            query: Query text
            candidates: List of candidate texts to rerank
            top_k: Return only top K results (None = all)

        Returns:
            List of (original_index, score) tuples sorted by score descending

        Raises:
            RuntimeError: If BGE not available in current tier or not loaded

        Example:
            >>> candidates = ['related doc', 'near-miss doc', 'unrelated doc']
            >>> ranked = embedder.rerank_with_bge('query', candidates, top_k=2)
            >>> # Returns: [(0, 0.95), (1, 0.72)]  # (index, score)
        """
        # Check if tier supports BGE reranking
        tier_requirements = self.tier_detector.TIER_REQUIREMENTS[self.current_tier]
        if not tier_requirements.features.get("reranking", False):
            raise RuntimeError(
                f"BGE reranking not available in {self.current_tier.value} tier. "
                f"Available features: {tier_requirements.features}"
            )

        if not self.bge_available:
            raise RuntimeError(
                "BGE-Reranker required by tier but not available. "
                "Run: python3 tmp_model_tests/scripts/download_bge_reranker.py"
            )

        if not self._models_loaded or self.bge_model is None:
            raise RuntimeError("BGE-Reranker not loaded. Call warm_up() first.")

        if not candidates:
            return []

        # Create query-candidate pairs
        pairs = [[query, candidate] for candidate in candidates]

        # Tokenize all pairs
        with torch.no_grad():
            inputs = self.bge_tokenizer(
                pairs,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512,
            )
            # Move to model device
            inputs = {k: v.to(self.bge_model.device) for k, v in inputs.items()}

            # Get logit scores (NOT probabilities)
            outputs = self.bge_model(**inputs)
            scores = outputs.logits.squeeze(-1).cpu().numpy()

            # Handle single candidate case
            if len(candidates) == 1:
                scores = np.array([scores])

        # Create (index, score) tuples
        indexed_scores = list(enumerate(scores))

        # Sort by score descending
        indexed_scores.sort(key=lambda x: x[1], reverse=True)

        # Apply top_k if specified
        if top_k is not None:
            indexed_scores = indexed_scores[:top_k]

        if indexed_scores:
            top_score = float(
                indexed_scores[0][1]
            )  # Convert numpy scalar to Python float
            logger.debug(
                f"BGE reranked {len(candidates)} candidates, top score: {top_score:.3f}"
            )

        return indexed_scores

    def get_cache_stats(self) -> dict[str, Any]:
        """
        Get cache statistics including tier information.

        Returns:
            Dict with cache size, memory usage estimate, and tier info
        """
        cache_size = len(self._embedding_cache)

        # Estimate memory usage (rough average of both models)
        avg_dim = (E5_DIMENSION + STARCODER_DIMENSION) / 2
        memory_bytes = cache_size * avg_dim * 4  # float32

        gpu_stats = {}
        if torch.cuda.is_available():
            gpu_stats = {
                "gpu_allocated_gb": torch.cuda.memory_allocated(0) / (1024**3),
                "gpu_reserved_gb": torch.cuda.memory_reserved(0) / (1024**3),
            }

        # Get tier information
        tier_info = self.tier_detector.get_tier_info()

        return {
            "cache_size": cache_size,
            "cache_memory_mb": memory_bytes / (1024 * 1024),
            "models_loaded": self._models_loaded,
            "tier_info": tier_info,
            "e5_dimension": E5_DIMENSION,
            "starcoder_dimension": STARCODER_DIMENSION,
            **gpu_stats,
        }


# Global singleton instance
_embedder_instance: DualModelEmbedder | None = None


def load_embedder_models(models: list[str], device: str, enable_reranking: bool) -> bool:
    """
    Load embedder models for a specific tier (used by TierDetector).

    This is a simplified interface for tier detection. The actual model loading
    happens lazily in DualModelEmbedder when models are first used.

    Args:
        models: List of model names to load (e.g., ["e5", "starcoder2"])
        device: Device to load models on ("cuda" or "cpu")
        enable_reranking: Whether to enable BGE reranking

    Returns:
        bool: True if models can be loaded, False otherwise

    Raises:
        RuntimeError: For out-of-memory or other loading errors
    """
    from pathlib import Path

    from prism_mcp.utils.config import get_config

    config = get_config()
    base_path = Path(config.data.base_path).expanduser()
    models_dir = config.data.models_dir

    # Check if required models exist
    for model_name in models:
        if model_name == "e5":
            model_path = base_path / models_dir / "e5-mistral-7b-instruct"
        elif model_name == "starcoder2":
            model_path = base_path / models_dir / "starcoder2-7b"
        elif model_name == "bge":
            model_path = base_path / models_dir / "bge-reranker-v2-m3"
        else:
            logger.warning(f"Unknown model: {model_name}")
            continue

        if not model_path.exists():
            logger.warning(f"Model not found: {model_path}")
            return False

    # Models exist and can be loaded
    return True


def get_embedder() -> DualModelEmbedder:
    """
    Get the global dual-model embedder instance (singleton pattern).

    Returns:
        DualModelEmbedder instance

    Raises:
        RuntimeError: If TIER_1_FULL not available
    """
    global _embedder_instance
    if _embedder_instance is None:
        _embedder_instance = DualModelEmbedder()
    return _embedder_instance


# Backward compatibility aliases
CodeEmbedder = DualModelEmbedder  # For existing code
