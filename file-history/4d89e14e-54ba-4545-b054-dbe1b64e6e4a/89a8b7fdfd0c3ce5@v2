"""
User Prompt Enricher - Fast context injection for user-prompt-submit hook.

Injects relevant context into user prompts based on:
- Active task/phase/complexity (Tier 1)
- Query type and pre-warmed patterns (Tier 2)
- Conversational state (Tier 3)

CRITICAL: Must be <10ms total latency.
- All data read from pre-warmed Redis cache
- No PRISM retrieval in hook
- No Neo4j queries in hook
- Simple keyword classification only

NO DEFAULTS - fail loud on errors.
"""

import json
import logging
import time
from pathlib import Path

from prism_mcp.storage.redis_cache import RedisCache
from prism_mcp.utils.concept_extractor import ConceptExtractor

logger = logging.getLogger(__name__)


class UserPromptEnricher:
    """
    Fast context injection for user-prompt-submit hook.

    Reads from pre-warmed Redis cache (ContextWarmer).
    No expensive operations in hook - all sub-millisecond cache reads.

    4-Tier Context Injection:
    - Tier 1: Always relevant (task, phase, file, components)
    - Tier 2: Query-triggered patterns (bugs, implementation, testing, architecture)
    - Tier 3: Conversational state (last topic for pronoun resolution)
    - Tier 4: Personal knowledge (preferences, lessons, tech opinions, threads)

    Size limits:
    - MAX 500 chars total injection
    - MAX 30% of original prompt length
    - Strict prioritization and truncation

    Escape hatch:
    - User can prefix query with @raw to disable enrichment
    """

    # Keywords for query classification
    BUG_KEYWORDS = {"bug", "error", "fix", "broken", "crash", "fail", "exception"}
    IMPLEMENT_KEYWORDS = {"implement", "add", "create", "build", "write", "develop"}
    TEST_KEYWORDS = {"test", "verify", "validate", "check", "assert"}
    ARCH_KEYWORDS = {"why", "decision", "architecture", "design", "choose", "chose"}
    EXPLORE_KEYWORDS = {"how", "what", "where", "explain", "tell me", "show me"}

    # Keywords indicating conversational (non-technical) queries
    CONVERSATIONAL_KEYWORDS = {
        "hey", "hello", "hi", "thanks", "thank you", "please",
        "can you help", "what should", "how do i use"
    }

    def __init__(self):
        """Initialize user prompt enricher."""
        self.redis = RedisCache()
        self.concept_extractor = ConceptExtractor()

        logger.info("UserPromptEnricher initialized")

    def enrich(
        self,
        user_query: str,
        session_id: str
    ) -> str:
        """
        Enrich user query with cached context.

        PERFORMANCE CRITICAL: Must complete in <10ms.

        Args:
            user_query: Original user query
            session_id: Session ID

        Returns:
            Enriched query with context injection, or original if no context

        Raises:
            RuntimeError: If required params missing
        """
        if not user_query:
            raise RuntimeError("user_query required")
        if not session_id:
            raise RuntimeError("session_id required")

        start_time = time.time()

        try:
            # Check escape hatch
            if user_query.startswith("@raw "):
                logger.debug("@raw escape hatch detected, skipping enrichment")
                return user_query[5:]

            # Check if conversational query (skip enrichment)
            if self._is_conversational(user_query):
                logger.debug("Conversational query detected, skipping enrichment")
                return user_query

            # Build context from cache
            context_parts = []

            # Tier 1: Always relevant context (if exists)
            tier1 = self._build_tier1_context(session_id)
            if tier1:
                context_parts.extend(tier1)

            # Tier 2: Query-triggered patterns
            query_type = self._classify_query(user_query)
            if query_type:
                tier2 = self._build_tier2_context(session_id, query_type)
                if tier2:
                    context_parts.extend(tier2)

            # Tier 3: Conversational state
            if self._is_pronoun_query(user_query):
                tier3 = self._build_tier3_context(session_id)
                if tier3:
                    context_parts.extend(tier3)

            # Tier 4: Personal context (preferences, lessons, tech opinions, threads)
            tier4 = self._build_tier4_context(session_id, query_type)
            if tier4:
                context_parts.extend(tier4)

            # No context to inject
            if not context_parts:
                elapsed = (time.time() - start_time) * 1000
                logger.debug(f"No context to inject ({elapsed:.1f}ms)")
                return user_query

            # Assemble context block
            context_block = "\n".join(context_parts)

            # Apply size limits
            if len(context_block) > 500:
                context_block = self._truncate_smart(context_block, 500)

            # Apply 30% rule
            max_length = int(len(user_query) * 0.3)
            if len(context_block) > max_length:
                logger.debug(f"Context too large ({len(context_block)} > 30% of {len(user_query)}), skipping")
                return user_query

            # Build enriched query
            enriched = f"{user_query}\n\n<!-- AUTO_CONTEXT -->\n{context_block}"

            elapsed = (time.time() - start_time) * 1000
            logger.info(f"Enriched query with {len(context_parts)} context items ({elapsed:.1f}ms)")

            return enriched

        except Exception as e:
            # NEVER crash the hook - return original query on error
            logger.error(f"Error enriching query: {e}")
            return user_query

    def _build_tier1_context(self, session_id: str) -> list[str]:
        """
        Build Tier 1 (always relevant) context from cache.

        Includes:
        - Active task
        - Current phase
        - Current file/symbol
        - Component dependencies

        Args:
            session_id: Session ID

        Returns:
            List of context strings (max 5 items)
        """
        context = []

        # Get task context
        task_json = self.redis.client.get(f"context:{session_id}:task")
        if task_json:
            task = json.loads(task_json)
            context.append(f"[TASK: {task['task_id']}]")
            context.append(f"[PHASE: {task['phase']}]")

            # Add components (top 3)
            components = task.get("components", [])[:3]
            if components:
                context.append(f"[COMPONENTS: {', '.join(components)}]")

        # Get current file context
        file_json = self.redis.client.get(f"context:{session_id}:current_file")
        if file_json:
            file_ctx = json.loads(file_json)

            # Check if context is recent (not stale)
            age = time.time() - file_ctx.get("timestamp", 0)
            if age < 600:  # 10 minutes
                file_path = file_ctx.get("file_path", "")
                line = file_ctx.get("line")
                symbol = file_ctx.get("symbol")

                # Format file context
                file_str = f"[FILE: {Path(file_path).name}"
                if line:
                    file_str += f":{line}"
                if symbol:
                    file_str += f" ({symbol})"
                file_str += "]"

                context.append(file_str)

        return context[:5]  # Max 5 items

    def _build_tier2_context(
        self,
        session_id: str,
        query_type: str
    ) -> list[str]:
        """
        Build Tier 2 (query-triggered) context from cache.

        Looks for pre-warmed patterns matching query type.

        Args:
            session_id: Session ID
            query_type: Query type (bugs/implementation/testing/architecture)

        Returns:
            List of pattern context strings (max 3)
        """
        context = []

        # Get current task to find component
        task_json = self.redis.client.get(f"context:{session_id}:task")
        if not task_json:
            return context

        task = json.loads(task_json)
        component = task.get("components", [None])[0]
        phase = task.get("phase", "implementation")

        if not component:
            return context

        # Map query type to phase pattern key
        pattern_key = f"patterns:{session_id}:{component}:{phase}"

        # Get patterns from cache
        patterns_json = self.redis.client.get(pattern_key)
        if not patterns_json:
            return context

        patterns = json.loads(patterns_json)

        # Add top 2 patterns
        for pattern in patterns[:2]:
            content = pattern.get("content", "")
            if len(content) > 80:
                content = content[:77] + "..."
            context.append(f"[PATTERN: {content}]")

        return context

    def _build_tier3_context(self, session_id: str) -> list[str]:
        """
        Build Tier 3 (conversational state) context from cache.

        Includes:
        - Last discussed topic (for pronoun resolution)

        Args:
            session_id: Session ID

        Returns:
            List of conversational context strings (max 1)
        """
        context = []

        # Get last topic
        last_topic = self.redis.client.get(f"context:{session_id}:last_topic")
        if last_topic:
            # Decode if bytes
            if isinstance(last_topic, bytes):
                last_topic = last_topic.decode("utf-8")

            context.append(f"[REFERS_TO: {last_topic}]")

        return context

    def _build_tier4_context(
        self,
        session_id: str,
        query_type: str | None = None
    ) -> list[str]:
        """
        Build Tier 4 (personal knowledge) context from cache.

        Includes:
        - Relevant preferences (top 2)
        - Recent lessons learned (top 1)
        - Tech opinions (avoid/prefer)
        - Active threads (if query-triggered)

        Args:
            session_id: Session ID
            query_type: Query type (optional, affects what personal context to include)

        Returns:
            List of personal context strings (max 4)
        """
        context = []

        # 1. Preferences (always include top 2 if available)
        preferences_json = self.redis.client.get(f"personal:{session_id}:preferences")
        if preferences_json:
            try:
                preferences = json.loads(preferences_json)
                for pref in preferences[:2]:  # Top 2 only
                    value = pref.get("value", "")
                    scope = pref.get("scope", "global")
                    # Format: [PREF(scope): value]
                    if len(value) > 50:
                        value = value[:47] + "..."
                    context.append(f"[PREF({scope}): {value}]")
            except Exception as e:
                logger.debug(f"Failed to parse preferences: {e}")

        # 2. Lessons learned (include top 1 for bugs/errors)
        if query_type in ["bugs", "implementation"]:
            lessons_json = self.redis.client.get(f"personal:{session_id}:lessons")
            if lessons_json:
                try:
                    lessons = json.loads(lessons_json)
                    if lessons:
                        lesson = lessons[0]
                        content = lesson.get("content", "")
                        if len(content) > 60:
                            content = content[:57] + "..."
                        context.append(f"[LESSON: {content}]")
                except Exception as e:
                    logger.debug(f"Failed to parse lessons: {e}")

        # 3. Tech opinions (include for architecture/implementation queries)
        if query_type in ["architecture", "implementation"]:
            opinions_json = self.redis.client.get(f"personal:{session_id}:tech_opinions")
            if opinions_json:
                try:
                    opinions = json.loads(opinions_json)
                    # Show top avoided tech (warning)
                    avoided = [op for op in opinions if op.get("sentiment") == "avoid"]
                    if avoided:
                        tech_name = avoided[0].get("tech_name", "")
                        context.append(f"[AVOID: {tech_name}]")
                except Exception as e:
                    logger.debug(f"Failed to parse tech opinions: {e}")

        # 4. Active threads (include for explore/conversational queries)
        if query_type in ["explore", None]:
            threads_json = self.redis.client.get(f"personal:{session_id}:active_threads")
            if threads_json:
                try:
                    threads = json.loads(threads_json)
                    if threads:
                        thread = threads[0]  # Most recent active thread
                        topic = thread.get("topic", "")
                        if topic:
                            context.append(f"[THREAD: {topic}]")
                except Exception as e:
                    logger.debug(f"Failed to parse threads: {e}")

        return context[:4]  # Max 4 personal context items

    def _classify_query(self, query: str) -> str | None:
        """
        Classify query type using simple keyword matching.

        FAST: <1ms keyword lookup.

        Args:
            query: User query

        Returns:
            Query type (bugs/implementation/testing/architecture/explore) or None
        """
        q_lower = query.lower()

        # Check each category
        if any(kw in q_lower for kw in self.BUG_KEYWORDS):
            return "bugs"
        elif any(kw in q_lower for kw in self.IMPLEMENT_KEYWORDS):
            return "implementation"
        elif any(kw in q_lower for kw in self.TEST_KEYWORDS):
            return "testing"
        elif any(kw in q_lower for kw in self.ARCH_KEYWORDS):
            return "architecture"
        elif any(kw in q_lower for kw in self.EXPLORE_KEYWORDS):
            return "explore"

        return None

    def _is_conversational(self, query: str) -> bool:
        """
        Detect conversational (non-technical) queries.

        Args:
            query: User query

        Returns:
            True if conversational query
        """
        q_lower = query.lower()
        return any(kw in q_lower for kw in self.CONVERSATIONAL_KEYWORDS)

    def _is_pronoun_query(self, query: str) -> bool:
        """
        Detect queries with pronouns needing resolution.

        Examples: "fix it", "update that", "change this"

        Args:
            query: User query

        Returns:
            True if query contains pronouns
        """
        q_lower = query.lower()
        pronouns = {"it", "that", "this", "them", "those", "these"}

        # Check if query contains pronouns
        words = q_lower.split()
        return any(word in pronouns for word in words)

    def _truncate_smart(self, text: str, max_length: int) -> str:
        """
        Smart truncation - keep highest priority items.

        Priority order:
        1. [TASK:...] - Always keep
        2. [PHASE:...] - Always keep
        3. [FILE:...] - Keep if possible
        4. [PATTERN:...] - Truncate if needed
        5. [COMPONENTS:...] - Drop if needed

        Args:
            text: Context text
            max_length: Max length in chars

        Returns:
            Truncated text
        """
        if len(text) <= max_length:
            return text

        lines = text.split("\n")

        # Prioritize lines
        essential = []  # TASK, PHASE
        important = []  # FILE, REFERS_TO, PREF (personal preferences)
        optional = []   # PATTERN, COMPONENTS, LESSON, AVOID, THREAD

        for line in lines:
            if "[TASK:" in line or "[PHASE:" in line:
                essential.append(line)
            elif "[FILE:" in line or "[REFERS_TO:" in line or "[PREF" in line:
                important.append(line)
            else:
                optional.append(line)

        # Build result respecting priority
        result = essential + important + optional

        # Join and truncate
        joined = "\n".join(result)
        if len(joined) <= max_length:
            return joined

        # Progressive removal
        if optional:
            result = essential + important
            joined = "\n".join(result)
            if len(joined) <= max_length:
                return joined

        if important:
            result = essential
            joined = "\n".join(result)
            return joined[:max_length]

        return joined[:max_length]


def get_user_prompt_enricher() -> UserPromptEnricher:
    """
    Get singleton user prompt enricher instance.

    Returns:
        UserPromptEnricher instance
    """
    return UserPromptEnricher()
