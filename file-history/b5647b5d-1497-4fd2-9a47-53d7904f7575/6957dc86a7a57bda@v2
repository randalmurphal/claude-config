"""
Memory engine with tiered storage for PRISM.

Implements four-tier memory hierarchy:
- ANCHORS: Critical, user-frustrated patterns (never forget)
- LONGTERM: Stable patterns and preferences
- EPISODIC: Recent session memories
- WORKING: Current task context

NO DEFAULTS - all tiers and thresholds configured.
"""

import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import numpy as np

from prism_mcp.models.embedder import get_embedder
from prism_mcp.storage.qdrant_manager import (
    QdrantManager,
    VectorPoint,
    MEMORY_TYPE_TO_COLLECTION,
    COLLECTION_E5,
    COLLECTION_STARCODER
)
from prism_mcp.storage.neo4j_manager import Neo4jManager, GraphNode, GraphRelationship
from prism_mcp.utils.config import get_config

logger = logging.getLogger(__name__)


class MemoryTier(Enum):
    """Memory storage tiers by importance."""
    ANCHORS = "anchors"      # Critical knowledge, high frustration items
    LONGTERM = "longterm"    # Stable patterns and preferences
    EPISODIC = "episodic"    # Recent activities and discoveries
    WORKING = "working"      # Current session context


@dataclass
class Memory:
    """
    A memory with metadata and tier assignment.

    Memories naturally promote/demote based on usage.
    """
    memory_id: str
    content: str
    memory_type: str  # pattern, correction, preference, discovery
    tier: MemoryTier
    embedding: np.ndarray

    # Metadata
    created_at: datetime
    last_accessed: datetime
    access_count: int = 0
    frustration_score: float = 0.0  # 0-1, higher = more user frustration

    # Retrieval metadata (Phase 3.2 - for adaptive thresholds)
    similarity_score: Optional[float] = None  # Similarity score from retrieval

    # Context
    project_id: Optional[str] = None
    branch: Optional[str] = None
    session_id: Optional[str] = None

    # Code indexing fields (for code symbols)
    file_path: Optional[str] = None
    symbol_name: Optional[str] = None
    symbol_type: Optional[str] = None
    line_start: Optional[int] = None
    line_end: Optional[int] = None
    signature: Optional[str] = None
    git_sha: Optional[str] = None

    # Intelligent retrieval context fields
    applicable_roles: Optional[List[str]] = None
    applicable_task_types: Optional[List[str]] = None
    applicable_phases: Optional[List[str]] = None

    # Relationships
    related_memories: List[str] = field(default_factory=list)
    derived_from: Optional[str] = None  # Parent memory if derived

    def access(self):
        """Record memory access."""
        self.access_count += 1
        self.last_accessed = datetime.now()

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        return {
            "memory_id": self.memory_id,
            "content": self.content,
            "memory_type": self.memory_type,
            "tier": self.tier.value,
            "created_at": self.created_at.isoformat(),
            "last_accessed": self.last_accessed.isoformat(),
            "access_count": self.access_count,
            "frustration_score": self.frustration_score,
            "similarity_score": self.similarity_score,  # Include similarity score from retrieval
            "project_id": self.project_id,
            "branch": self.branch,
            "session_id": self.session_id,
            "file_path": self.file_path,
            "symbol_name": self.symbol_name,
            "symbol_type": self.symbol_type,
            "line_start": self.line_start,
            "line_end": self.line_end,
            "signature": self.signature,
            "git_sha": self.git_sha,
            "applicable_roles": self.applicable_roles,
            "applicable_task_types": self.applicable_task_types,
            "applicable_phases": self.applicable_phases,
            "related_memories": self.related_memories,
            "derived_from": self.derived_from
        }


class MemoryEngine:
    """
    Hierarchical memory management with natural promotion/demotion.

    Single responsibility: Store and retrieve memories with proper tiering.
    NO hardcoded thresholds - all driven by configuration or usage patterns.
    """

    def __init__(self):
        """
        Initialize memory engine from configuration.

        Raises:
            RuntimeError: If configuration missing or invalid
        """
        config = get_config()

        # Initialize services
        self.embedder = get_embedder()
        self.qdrant = QdrantManager()
        self.neo4j = Neo4jManager()

        # Phase 3.2: Adaptive thresholds support
        self.enable_adaptive_thresholds = config.adaptive_thresholds.enable_adaptive_thresholds
        self.default_threshold = config.adaptive_thresholds.default_threshold
        self._adaptive_thresholds: Dict[str, float] = {}

        if self.enable_adaptive_thresholds:
            try:
                self._load_adaptive_thresholds()
                logger.info(f"Loaded {len(self._adaptive_thresholds)} adaptive thresholds from Neo4j")
            except Exception as e:
                # FALLBACK DOCUMENTED: Adaptive threshold loading failed
                # System continues with configured default_threshold (from config.yaml)
                # This is NOT a silent default - default_threshold is explicitly configured
                logger.warning(
                    f"Failed to load adaptive thresholds from Neo4j: {e}\n"
                    f"Fallback: Using configured default_threshold={self.default_threshold} for all memories.\n"
                    f"This is less precise but system remains operational."
                )

        # NO LOCAL MEMORY STORAGE - query from databases only
        # This avoids thread safety issues and follows our philosophy
        # All memories are stored in Qdrant/Neo4j and queried on demand

        # Promotion thresholds - MUST be configured
        # Direct access - will crash if missing (NO DEFAULTS philosophy)
        self.promotion_thresholds = {
            'to_anchors_frustration': config.memory.to_anchors_frustration,  # Frustration level for ANCHORS
            'to_longterm_accesses': config.memory.to_longterm_accesses,      # Access count for LONGTERM
            'to_episodic_age_hours': config.memory.to_episodic_age_hours,    # Age for EPISODIC demotion
            'working_ttl_hours': config.memory.working_ttl_hours             # TTL for WORKING memory
        }

        # Memory importance weights - MUST be configured (architectural decisions)
        # Direct access - will crash if missing (NO DEFAULTS philosophy)
        self.importance_weights = {
            'graph': config.memory.importance_weights.graph,         # PageRank/centrality weight
            'access': config.memory.importance_weights.access,       # Access frequency weight
            'frustration': config.memory.importance_weights.frustration  # User frustration weight
        }

        # Initialize graph projection for GDS algorithms
        # GDS is REQUIRED - no fallback behavior
        self._ensure_graph_projection()

        logger.info("MemoryEngine initialized with tiered storage")

    def store_memory(
        self,
        content: str,
        memory_type: str,
        tier: Optional[MemoryTier] = None,
        frustration_score: float = 0.0,
        project_id: Optional[str] = None,
        branch: Optional[str] = None,
        session_id: Optional[str] = None,
        related_to: Optional[List[str]] = None,
        file_path: Optional[str] = None,
        symbol_name: Optional[str] = None,
        symbol_type: Optional[str] = None,
        line_start: Optional[int] = None,
        line_end: Optional[int] = None,
        signature: Optional[str] = None,
        git_sha: Optional[str] = None
    ) -> Memory:
        """
        Store a new memory with automatic tier assignment.

        Args:
            content: Memory content
            memory_type: Type of memory (pattern, correction, etc.)
            tier: Explicit tier assignment (optional)
            frustration_score: User frustration level (0-1)
            project_id: Project context
            branch: Branch context
            session_id: Session context
            related_to: Related memory IDs
            file_path: File path (for code symbols)
            symbol_name: Symbol name (for code symbols)
            symbol_type: Symbol type (for code symbols)
            line_start: Start line (for code symbols)
            line_end: End line (for code symbols)
            signature: Function signature (for code symbols)
            git_sha: Git commit SHA (for code symbols)

        Returns:
            Created Memory object
        """
        # Generate embedding with memory_type for model routing
        embedding = self.embedder.generate_embedding(content, memory_type=memory_type)

        # Auto-assign tier if not specified
        if tier is None:
            tier = self._determine_tier(frustration_score, memory_type)

        # Create memory
        memory = Memory(
            memory_id=self._generate_memory_id(),
            content=content,
            memory_type=memory_type,
            tier=tier,
            embedding=embedding,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            frustration_score=frustration_score,
            project_id=project_id,
            branch=branch,
            session_id=session_id,
            related_memories=related_to or [],
            file_path=file_path,
            symbol_name=symbol_name,
            symbol_type=symbol_type,
            line_start=line_start,
            line_end=line_end,
            signature=signature,
            git_sha=git_sha
        )

        # Store ONLY in databases, not locally
        # Store in Qdrant for vector search
        self._store_in_qdrant(memory)

        # Store in Neo4j for tracking and relationships
        # Always create node for tier management and session tracking
        self._store_in_neo4j(memory)

        # Store relationships if any
        if related_to:
            self._store_relationships_only(memory, related_to)

        logger.info(
            f"Stored memory {memory.memory_id} in {tier.value} tier "
            f"(frustration: {frustration_score:.2f})"
        )

        return memory

    def retrieve_relevant(
        self,
        query: str,
        memory_type: str,
        limit: int = 10,
        tier_filter: Optional[MemoryTier] = None,
        project_id: Optional[str] = None,
        branch: Optional[str] = None,
        augment: bool = False,
        rerank: bool = False,
        score_threshold: Optional[float] = None
    ) -> List[Memory]:
        """
        Retrieve relevant memories using semantic search.

        Phase 2.3: Now supports query augmentation and BGE reranking.
        Phase 2.4: Now supports score threshold filtering (optimal: 0.72).

        Args:
            query: Search query
            memory_type: Type of memory to retrieve (routes to correct collection)
            limit: Maximum results
            tier_filter: Filter by tier
            project_id: Filter by project
            branch: Filter by branch
            augment: Apply query augmentation (Phase 2.1, E5 only, +33% precision)
            rerank: Apply BGE reranking (Phase 2.2, optional, +0-50% precision)
            score_threshold: Minimum similarity score (Phase 2.4, optimal: 0.72)

        Returns:
            Relevant memories sorted by relevance
        """
        # Determine collection from memory_type (tier-aware routing)
        # Routes to Voyage collections if using Premium API, E5/StarCoder2 otherwise
        collection = self.qdrant.get_collection_for_memory_type(memory_type)

        # Generate query embedding with augmentation if requested
        # Phase 2.3: Use augmentation for E5 queries if enabled
        if augment and collection == COLLECTION_E5:
            query_embedding = self.embedder.embed_with_e5(query, augment=True)
        else:
            query_embedding = self.embedder.generate_embedding(query, memory_type=memory_type)

        # Build filter conditions
        filter_conditions = {}
        if tier_filter:
            filter_conditions["tier"] = tier_filter.value
        if project_id:
            filter_conditions["project_id"] = project_id
        if branch:
            filter_conditions["branch"] = branch

        # Phase 3.2: Use adaptive threshold if none specified
        if score_threshold is None and self.enable_adaptive_thresholds:
            score_threshold = self._get_threshold_for_context(memory_type)

        # Search in appropriate collection (memories_e5 or memories_starcoder)
        # Phase 2.4: Apply score threshold if specified (optimal: 0.72)
        # Phase 3.2: Uses adaptive threshold if enabled
        results = self.qdrant.search(
            collection_name=collection,
            query_vector=query_embedding,
            limit=limit,
            filter_conditions=filter_conditions if filter_conditions else None,
            with_vectors=True,
            score_threshold=score_threshold
        )

        # Convert results to Memory objects
        memories = []
        for result in results:
            memory_id = result['payload']['memory_id']
            tier_name = result['payload']['tier']
            tier = MemoryTier(tier_name)

            # Reconstruct memory from payload
            memory_data = result['payload']
            # Get embedding from result vector (not payload)
            embedding_vector = result.get('vector', [])
            memory = Memory(
                memory_id=memory_data['memory_id'],
                content=memory_data['content'],
                memory_type=memory_data['memory_type'],
                tier=MemoryTier(memory_data['tier']),
                embedding=np.array(embedding_vector) if embedding_vector else np.array([]),
                created_at=datetime.fromisoformat(memory_data['created_at']),
                last_accessed=datetime.fromisoformat(memory_data['last_accessed']),
                access_count=memory_data['access_count'],
                frustration_score=memory_data['frustration_score'],
                similarity_score=result.get('score'),  # Phase 3.2: Store retrieval score
                project_id=memory_data['project_id'] if 'project_id' in memory_data else None,
                branch=memory_data['branch'] if 'branch' in memory_data else None,
                session_id=memory_data['session_id'] if 'session_id' in memory_data else None,
                related_memories=memory_data['related_memories'],
                derived_from=memory_data['derived_from'] if 'derived_from' in memory_data else None,
                file_path=memory_data.get('file_path'),
                symbol_name=memory_data.get('symbol_name'),
                symbol_type=memory_data.get('symbol_type'),
                line_start=memory_data.get('line_start'),
                line_end=memory_data.get('line_end'),
                signature=memory_data.get('signature'),
                git_sha=memory_data.get('git_sha')
            )
            memory.access()  # Record access
            memories.append(memory)

        # Phase 2.3: Apply BGE reranking if requested and available
        if rerank and self.embedder.bge_available and memories:
            logger.info(f"Reranking {len(memories)} results with BGE")
            # Get memory contents for reranking
            candidates = [m.content for m in memories]

            # Rerank with BGE
            try:
                ranked_indices = self.embedder.rerank_with_bge(query, candidates)
                # Reorder memories based on BGE ranking
                memories = [memories[idx] for idx, score in ranked_indices]
                logger.debug(f"Reranked {len(memories)} memories, top BGE score: {ranked_indices[0][1]:.3f}")
            except Exception as e:
                logger.warning(f"BGE reranking failed: {e}, continuing with original order")

        # Check for promotions after access
        for memory in memories:
            self._check_promotion(memory)

        return memories

    def _load_adaptive_thresholds(self):
        """
        Load adaptive thresholds from Neo4j.

        Phase 3.2: Loads AdaptiveThreshold nodes for context-specific thresholds.

        Raises:
            RuntimeError: If Neo4j unavailable (fail loud)
        """
        try:
            from prism_mcp.core.threshold_learner import get_threshold_learner

            learner = get_threshold_learner()
            self._adaptive_thresholds = learner.get_learned_thresholds()

            logger.debug(f"Loaded adaptive thresholds: {list(self._adaptive_thresholds.keys())}")

        except Exception as e:
            raise RuntimeError(f"Failed to load adaptive thresholds from Neo4j: {e}") from e

    def _get_threshold_for_context(self, memory_type: str) -> float:
        """
        Get threshold for the given context (model based on memory_type).

        Phase 3.2: Uses adaptive threshold if available, otherwise default.

        Args:
            memory_type: Type of memory being retrieved

        Returns:
            Threshold value (0.0-1.0)
        """
        if not self.enable_adaptive_thresholds:
            return self.default_threshold

        # Determine model from memory_type
        e5_types = {'research_note', 'decision', 'gotcha', 'guidance'}
        context_key = "e5" if memory_type in e5_types else "starcoder"

        # Check if we have a learned threshold for this context
        if context_key in self._adaptive_thresholds:
            threshold = self._adaptive_thresholds[context_key]
            logger.debug(f"Using adaptive threshold for {context_key}: {threshold:.2f}")
            return threshold

        # Fall back to default
        logger.debug(f"No adaptive threshold for {context_key}, using default: {self.default_threshold:.2f}")
        return self.default_threshold

    def retrieve_parallel(
        self,
        query: str,
        limit: int = 10,
        tier_filter: Optional[MemoryTier] = None,
        project_id: Optional[str] = None,
        branch: Optional[str] = None,
        augment: bool = False,
        rerank: bool = False,
        score_threshold: Optional[float] = None
    ) -> List[Memory]:
        """
        Retrieve relevant memories from BOTH collections in parallel.

        Phase 2.3: Now supports query augmentation and BGE reranking.
        Phase 2.4: Now supports score threshold filtering (optimal: 0.72).

        Queries both memories_e5 and memories_starcoder simultaneously,
        then merges results by score. Uses both E5 and StarCoder2 models
        concurrently to prove dual-model capability.

        Args:
            query: Search query (will be embedded with both models)
            limit: Maximum total results
            tier_filter: Filter by tier
            project_id: Filter by project
            branch: Filter by branch
            augment: Apply query augmentation (Phase 2.1, E5 only, +33% precision)
            rerank: Apply BGE reranking (Phase 2.2, optional, +0-50% precision)
            score_threshold: Minimum similarity score (Phase 2.4, optimal: 0.72)

        Returns:
            Merged memories from both collections, sorted by score
        """
        # Generate embeddings with both models
        # Phase 2.3: Use augmentation for E5 if enabled
        e5_embedding = self.embedder.embed_with_e5(query, augment=augment)
        starcoder_embedding = self.embedder.embed_with_starcoder(query)

        # Build filter conditions
        filter_conditions = {}
        if tier_filter:
            filter_conditions["tier"] = tier_filter.value
        if project_id:
            filter_conditions["project_id"] = project_id
        if branch:
            filter_conditions["branch"] = branch

        # Query both collections (CONCURRENT dual-model operation)
        # Phase 2.4: Apply score threshold if specified (optimal: 0.72)
        e5_results = self.qdrant.search(
            collection_name=COLLECTION_E5,
            query_vector=e5_embedding,
            limit=limit,
            filter_conditions=filter_conditions if filter_conditions else None,
            with_vectors=True,
            score_threshold=score_threshold
        )

        starcoder_results = self.qdrant.search(
            collection_name=COLLECTION_STARCODER,
            query_vector=starcoder_embedding,
            limit=limit,
            filter_conditions=filter_conditions if filter_conditions else None,
            with_vectors=True,
            score_threshold=score_threshold
        )

        # Merge results by score
        all_results = []
        for result in e5_results:
            all_results.append((result, result['score']))
        for result in starcoder_results:
            all_results.append((result, result['score']))

        # Sort by score descending
        all_results.sort(key=lambda x: x[1], reverse=True)

        # Take top limit results
        top_results = all_results[:limit]

        # Convert to Memory objects
        memories = []
        for result, score in top_results:
            memory_data = result['payload']
            embedding_vector = result.get('vector', [])

            memory = Memory(
                memory_id=memory_data['memory_id'],
                content=memory_data['content'],
                memory_type=memory_data['memory_type'],
                tier=MemoryTier(memory_data['tier']),
                embedding=np.array(embedding_vector) if embedding_vector else np.array([]),
                created_at=datetime.fromisoformat(memory_data['created_at']),
                last_accessed=datetime.fromisoformat(memory_data['last_accessed']),
                access_count=memory_data['access_count'],
                frustration_score=memory_data['frustration_score'],
                similarity_score=result.get('score'),  # Phase 3.2: Store retrieval score
                project_id=memory_data['project_id'] if 'project_id' in memory_data else None,
                branch=memory_data['branch'] if 'branch' in memory_data else None,
                session_id=memory_data['session_id'] if 'session_id' in memory_data else None,
                related_memories=memory_data['related_memories'],
                derived_from=memory_data['derived_from'] if 'derived_from' in memory_data else None,
                file_path=memory_data.get('file_path'),
                symbol_name=memory_data.get('symbol_name'),
                symbol_type=memory_data.get('symbol_type'),
                line_start=memory_data.get('line_start'),
                line_end=memory_data.get('line_end'),
                signature=memory_data.get('signature'),
                git_sha=memory_data.get('git_sha')
            )
            memory.access()
            memories.append(memory)

        # Phase 2.3: Apply BGE reranking if requested and available
        if rerank and self.embedder.bge_available and memories:
            logger.info(f"Reranking {len(memories)} parallel results with BGE")
            # Get memory contents for reranking
            candidates = [m.content for m in memories]

            # Rerank with BGE
            try:
                ranked_indices = self.embedder.rerank_with_bge(query, candidates)
                # Reorder memories based on BGE ranking
                memories = [memories[idx] for idx, score in ranked_indices]
                logger.debug(f"Reranked {len(memories)} memories, top BGE score: {ranked_indices[0][1]:.3f}")
            except Exception as e:
                logger.warning(f"BGE reranking failed: {e}, continuing with original order")

        # Check for promotions
        for memory in memories:
            self._check_promotion(memory)

        logger.info(
            f"Parallel retrieval: {len(e5_results)} from E5, "
            f"{len(starcoder_results)} from StarCoder2, "
            f"merged to {len(memories)} results"
        )

        return memories

    def _determine_tier(self, frustration_score: float, memory_type: str) -> MemoryTier:
        """
        Determine initial tier using database intelligence.

        Let the data tell us what's important, not hardcoded rules.

        Args:
            frustration_score: User frustration level
            memory_type: Type of memory

        Returns:
            Appropriate MemoryTier
        """
        # Use Neo4j to find similar memories and their tiers
        # Sample first 1000 memories for performance on large databases
        result = self.neo4j.query("""
            MATCH (m:Memory)
            WHERE m.type = $memory_type
            WITH m LIMIT 1000
            WITH m.tier as tier, avg(m.frustration_score) as avg_frustration
            ORDER BY avg_frustration DESC
            RETURN tier, avg_frustration
            LIMIT 1
        """, memory_type=memory_type)

        if result and frustration_score >= result[0]['avg_frustration']:
            # Use the tier of similar frustrated memories
            return MemoryTier(result[0]['tier'])

        # For new types, let importance emerge from usage
        # Start everything in WORKING and let it promote naturally
        return MemoryTier.WORKING

    def _check_promotion(self, memory: Memory):
        """
        Check if memory should be promoted using graph importance.

        Uses PageRank to determine importance naturally.

        Args:
            memory: Memory to check
        """

        # Calculate importance using Neo4j PageRank
        # First ensure graph projection exists
        try:
            self._ensure_graph_projection()

            # Check if graph projection exists (might not if no relationships)
            proj_exists = self.neo4j.query("""
                CALL gds.graph.exists('memory-graph')
                YIELD exists
                RETURN exists
            """)

            if proj_exists and proj_exists[0]['exists']:
                result = self.neo4j.query("""
                    MATCH (m:Memory {memory_id: $memory_id})
                    CALL gds.pageRank.stream('memory-graph')
                    YIELD nodeId, score
                    WHERE id(m) = nodeId
                    RETURN score
                """, memory_id=memory.memory_id)

                if result:
                    importance_score = result[0]['score']
                else:
                    importance_score = 0.1  # Default if not in graph
            else:
                # No graph projection - use simple importance without PageRank
                importance_score = 0.1  # Base importance without graph context
        except Exception as e:
            # GDS plugin not available or graph operations failed
            # Fallback to simple importance calculation (no PageRank)
            logger.warning(f"Graph projection error (GDS unavailable?): {e}. Using simple importance.")
            importance_score = 0.1  # Base importance without graph context

        # Also check access frequency relative to age
        age_hours = (datetime.now() - memory.created_at).total_seconds() / 3600
        if age_hours > 0:
            access_rate = memory.access_count / age_hours
        else:
            access_rate = memory.access_count

        # Combine importance metrics using configured weights
        combined_importance = (
            importance_score * self.importance_weights['graph'] +  # Graph importance
            min(access_rate / 10, 1.0) * self.importance_weights['access'] +  # Access frequency (normalized)
            memory.frustration_score * self.importance_weights['frustration']  # User frustration
        )

        # Natural tier boundaries emerge from data distribution
        # Get tier thresholds from percentiles of existing memories
        tier_boundaries = self._get_tier_boundaries()

        # Promote based on natural boundaries
        if combined_importance >= tier_boundaries['anchors']:
            if memory.tier != MemoryTier.ANCHORS:
                self._promote_memory(memory, MemoryTier.ANCHORS)
        elif combined_importance >= tier_boundaries['longterm']:
            if memory.tier != MemoryTier.LONGTERM:
                self._promote_memory(memory, MemoryTier.LONGTERM)
        elif combined_importance >= tier_boundaries['episodic']:
            if memory.tier != MemoryTier.EPISODIC:
                self._promote_memory(memory, MemoryTier.EPISODIC)

    def _promote_memory(self, memory: Memory, new_tier: MemoryTier):
        """
        Promote memory to a higher tier.

        Args:
            memory: Memory to promote
            new_tier: Target tier
        """
        old_tier = memory.tier

        # Update tier
        memory.tier = new_tier

        # Update ONLY in Qdrant, not local storage
        self._update_in_qdrant(memory)

        logger.info(f"Promoted memory {memory.memory_id} from {old_tier.value} to {new_tier.value}")

    def _store_in_qdrant(self, memory: Memory):
        """
        Store memory in appropriate Qdrant collection based on memory_type.
        Tier-aware: routes to Voyage (1024d) in Tier 0, E5/StarCoder2 (4096d/4608d) in Tier 1+.

        Args:
            memory: Memory to store
        """
        # Use tier-aware collection routing
        collection = self.qdrant.get_collection_for_memory_type(memory.memory_type)

        # Ensure collection exists
        self.qdrant.ensure_collection(collection)

        # Add collection to payload for tracking
        payload = memory.to_dict()
        payload['collection'] = collection

        point = VectorPoint(
            point_id=self._memory_id_to_uuid(memory.memory_id),
            vector=memory.embedding,
            payload=payload
        )

        self.qdrant.upsert(collection, [point])

    def _update_in_qdrant(self, memory: Memory):
        """
        Update memory in Qdrant after tier change.

        Args:
            memory: Memory to update
        """
        # Just re-store with updated payload
        self._store_in_qdrant(memory)

    def _store_in_neo4j(self, memory: Memory):
        """
        Store memory node in Neo4j for tracking and graph algorithms.

        Args:
            memory: Memory to store
        """
        # Create memory node with all tracking properties
        memory_node = GraphNode(
            node_id=memory.memory_id,
            node_type="Memory",
            properties={
                "memory_id": memory.memory_id,  # Explicit for queries
                "content": memory.content[:200],  # Truncate for graph storage
                "type": memory.memory_type,
                "tier": memory.tier.value,  # Use enum value directly (lowercase)
                "project": memory.project_id,
                "branch": memory.branch,
                "session_id": memory.session_id,  # Essential for session tracking
                "access_count": memory.access_count,
                "frustration_score": memory.frustration_score,
                "created_at": memory.created_at.isoformat(),
                "last_accessed": memory.last_accessed.isoformat()
            }
        )
        self.neo4j.create_node(memory_node, unique_property="memory_id")

    def _store_relationships_only(self, memory: Memory, related_ids: List[str]):
        """
        Store only the relationships (node already exists).

        Args:
            memory: New memory
            related_ids: Related memory IDs
        """
        # Create relationships
        for related_id in related_ids:
            rel = GraphRelationship(
                from_id=memory.memory_id,
                to_id=related_id,
                relationship_type="RELATED_TO",
                properties={"created_at": datetime.now().isoformat()}
            )
            self.neo4j.create_relationship(rel)

    def _store_relationships(self, memory: Memory, related_ids: List[str]):
        """
        Store memory relationships in Neo4j.
        DEPRECATED - use _store_in_neo4j and _store_relationships_only

        Args:
            memory: New memory
            related_ids: Related memory IDs
        """
        # Create memory node
        memory_node = GraphNode(
            node_id=memory.memory_id,
            node_type="Memory",
            properties={
                "content": memory.content[:200],  # Truncate for graph storage
                "type": memory.memory_type,
                "tier": memory.tier.value,
                "project": memory.project_id
            }
        )
        self.neo4j.create_node(memory_node)

        # Create relationships
        for related_id in related_ids:
            rel = GraphRelationship(
                from_id=memory.memory_id,
                to_id=related_id,
                relationship_type="RELATED_TO",
                properties={"created_at": datetime.now().isoformat()}
            )
            self.neo4j.create_relationship(rel)

    def _get_tier_boundaries(self) -> Dict[str, float]:
        """
        Get natural tier boundaries from data distribution.

        Returns percentiles of importance scores, not hardcoded values.
        """
        # Query all memory importance scores
        # Ensure graph projection exists
        self._ensure_graph_projection()

        # Check if graph projection exists
        proj_exists = self.neo4j.query("""
            CALL gds.graph.exists('memory-graph')
            YIELD exists
            RETURN exists
        """)

        if proj_exists and proj_exists[0]['exists']:
            result = self.neo4j.query("""
                CALL gds.pageRank.stream('memory-graph')
                YIELD nodeId, score
                WITH collect(score) as scores
                RETURN percentileCont(scores, 0.95) as anchors_threshold,
                       percentileCont(scores, 0.80) as longterm_threshold,
                       percentileCont(scores, 0.50) as episodic_threshold
            """)

            if result:
                return {
                    'anchors': result[0]['anchors_threshold'],
                    'longterm': result[0]['longterm_threshold'],
                    'episodic': result[0]['episodic_threshold']
                }

        # If no graph or no data yet, return very high thresholds (nothing promotes)
        return {
            'anchors': 0.95,
            'longterm': 0.80,
            'episodic': 0.50
        }

    def _generate_memory_id(self) -> str:
        """Generate unique memory ID."""
        import hashlib
        import time
        import random

        # Use timestamp with random component for uniqueness
        # NO DEFAULTS - no local state to track count
        unique_string = f"memory_{time.time()}_{random.randint(0, 999999)}"
        return hashlib.md5(unique_string.encode()).hexdigest()[:12]

    def _memory_id_to_uuid(self, memory_id: str) -> str:
        """
        Convert memory_id to UUID format for Qdrant.
        Handles both 12-char hex (legacy) and full UUID (preferences).

        Qdrant requires point IDs to be UUIDs or integers.
        We pad the 12-char hex to 32 chars and format as UUID.

        Args:
            memory_id: 12-character hex string OR full UUID

        Returns:
            UUID-formatted string (e.g., "f3a184e8-f770-0000-0000-000000000000")
        """
        # If already a UUID, return as-is
        if len(memory_id) == 36 and memory_id.count('-') == 4:
            return memory_id

        # Legacy 12-char format
        if len(memory_id) != 12:
            raise ValueError(f"memory_id must be 12 or 36 characters, got {len(memory_id)}")

        padded = memory_id + "0" * 20
        return f"{padded[0:8]}-{padded[8:12]}-{padded[12:16]}-{padded[16:20]}-{padded[20:32]}"

    def get_tier_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about memory tiers.

        Returns:
            Tier counts and statistics
        """
        stats = {}

        # Query statistics from database
        for tier in MemoryTier:
            # Count memories in this tier
            tier_result = self.neo4j.query("""
                MATCH (m:Memory {tier: $tier})
                RETURN count(m) as count,
                       avg(m.access_count) as avg_access,
                       max(m.access_count) as max_access,
                       avg(m.frustration_score) as avg_frustration,
                       max(m.frustration_score) as max_frustration
            """, tier=tier.value)

            if tier_result and tier_result[0]['count'] > 0:
                stats[tier.value] = {
                    "count": tier_result[0]['count'],
                    "avg_access_count": tier_result[0]['avg_access'] or 0,
                    "max_access_count": tier_result[0]['max_access'] or 0,
                    "avg_frustration": tier_result[0]['avg_frustration'] or 0,
                    "max_frustration": tier_result[0]['max_frustration'] or 0
                }
            else:
                stats[tier.value] = {"count": 0}

        return stats

    def cleanup_stale_memories(self):
        """
        Clean up stale WORKING memories based on TTL.

        This runs periodically to prevent memory bloat.
        """
        ttl_hours = self.promotion_thresholds['working_ttl_hours']
        cutoff_time = datetime.now() - timedelta(hours=ttl_hours)

        # Query stale memories from database
        stale_result = self.neo4j.query("""
            MATCH (m:Memory {tier: $tier})
            WHERE m.last_accessed < $cutoff
            RETURN m.memory_id as id
        """, tier=MemoryTier.WORKING.value, cutoff=cutoff_time.isoformat())

        stale_ids = [r['id'] for r in stale_result] if stale_result else []

        # Delete stale memories from databases only
        for memory_id in stale_ids:
            # Remove from Qdrant
            self.qdrant.delete_pattern(memory_id)
            # Remove from Neo4j
            self.neo4j.query("""
                MATCH (m:Memory {memory_id: $id})
                DELETE m
            """, id=memory_id)

        if stale_ids:
            logger.info(f"Cleaned up {len(stale_ids)} stale WORKING memories")

    def _ensure_graph_projection(self):
        """
        Ensure graph projection exists for GDS algorithms.

        Creates projection if not exists, updates if needed.
        """
        # Check if projection exists
        result = self.neo4j.query("""
            CALL gds.graph.exists('memory-graph')
            YIELD exists
            RETURN exists
        """)

        if result and result[0]['exists']:
            # Projection exists, optionally update it
            logger.debug("Graph projection 'memory-graph' already exists")
        else:
            # Create new projection
            self._create_graph_projection()

    def _create_graph_projection(self):
        """
        Create graph projection for memory analysis.
        Only create if we have both nodes AND relationships.
        """
        # Check if Memory nodes exist
        count_result = self.neo4j.query("MATCH (m:Memory) RETURN count(m) as count")
        node_count = count_result[0]['count'] if count_result else 0

        # Check if RELATED_TO relationships exist
        rel_result = self.neo4j.query("MATCH ()-[r:RELATED_TO]->() RETURN count(r) as count")
        rel_count = rel_result[0]['count'] if rel_result else 0

        if node_count == 0:
            logger.info("No Memory nodes exist yet - skipping graph projection")
            return  # OK to have no memories initially

        if rel_count == 0:
            logger.info(f"Have {node_count} Memory nodes but no relationships - skipping graph projection")
            return  # OK to have nodes without relationships initially

        # Drop existing projection if it exists
        # Use failIfMissing=false to avoid error if not exists
        self.neo4j.query("""
            CALL gds.graph.drop('memory-graph', false)
            YIELD graphName
            RETURN graphName
        """)

        # Create new projection - MUST succeed or crash
        result = self.neo4j.query("""
            CALL gds.graph.project(
                'memory-graph',
                'Memory',
                'RELATED_TO'
            )
            YIELD graphName, nodeCount, relationshipCount
            RETURN graphName, nodeCount, relationshipCount
        """)

        if not result:
            raise RuntimeError("Failed to create graph projection for memory analysis")

        logger.info(f"Created graph projection with {result[0]['nodeCount']} nodes, {result[0]['relationshipCount']} relationships")


def get_memory_engine() -> MemoryEngine:
    """Get MemoryEngine instance."""
    return MemoryEngine()