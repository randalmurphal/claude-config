"""
PRISM Engine - Unified memory and intelligence system.

Single unified brain for all PRISM operations:
- Memory storage/retrieval (tiered hierarchy)
- ADR management (architectural decisions)
- Code duplication detection
- Graph-based memory promotion

Architecture:
- Direct calls to storage (Qdrant, Neo4j)
- NO async (all operations are sequential)
- Tier-aware routing built-in
- All logic in one place
"""

import hashlib
import logging
import random
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Optional

import numpy as np

from prism_mcp.models.embedder import get_embedder
from prism_mcp.storage.neo4j_manager import (
    GraphNode,
    GraphRelationship,
    Neo4jManager,
)
from prism_mcp.storage.qdrant_manager import QdrantManager, VectorPoint
from prism_mcp.utils.config import get_config


logger = logging.getLogger(__name__)


class MemoryTier(Enum):
    """Memory storage tiers by importance."""

    ANCHORS = 'anchors'  # Critical knowledge, ADRs, high frustration
    LONGTERM = 'longterm'  # Stable patterns and preferences
    EPISODIC = 'episodic'  # Recent session memories
    WORKING = 'working'  # Current task context


@dataclass
class Memory:
    """
    A memory with metadata and tier assignment.

    Memories naturally promote/demote based on usage patterns.
    """

    memory_id: str
    content: str
    memory_type: str
    tier: MemoryTier
    embedding: np.ndarray

    # Metadata
    created_at: datetime
    last_accessed: datetime
    access_count: int = 0
    frustration_score: float = 0.0
    similarity_score: Optional[float] = None

    # Context
    project_id: Optional[str] = None
    branch: Optional[str] = None
    session_id: Optional[str] = None

    # Code indexing fields
    file_path: Optional[str] = None
    symbol_name: Optional[str] = None
    symbol_type: Optional[str] = None
    line_start: Optional[int] = None
    line_end: Optional[int] = None
    signature: Optional[str] = None
    git_sha: Optional[str] = None

    # Intelligent retrieval
    applicable_roles: Optional[list[str]] = None
    applicable_task_types: Optional[list[str]] = None
    applicable_phases: Optional[list[str]] = None

    # Relationships
    related_memories: list[str] = field(default_factory=list)
    derived_from: Optional[str] = None

    def access(self):
        """Record memory access."""
        self.access_count += 1
        self.last_accessed = datetime.now()

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for storage."""
        return {
            'memory_id': self.memory_id,
            'content': self.content,
            'memory_type': self.memory_type,
            'tier': self.tier.value,
            'created_at': self.created_at.isoformat(),
            'last_accessed': self.last_accessed.isoformat(),
            'access_count': self.access_count,
            'frustration_score': self.frustration_score,
            'similarity_score': self.similarity_score,
            'project_id': self.project_id,
            'branch': self.branch,
            'session_id': self.session_id,
            'file_path': self.file_path,
            'symbol_name': self.symbol_name,
            'symbol_type': self.symbol_type,
            'line_start': self.line_start,
            'line_end': self.line_end,
            'signature': self.signature,
            'git_sha': self.git_sha,
            'applicable_roles': self.applicable_roles,
            'applicable_task_types': self.applicable_task_types,
            'applicable_phases': self.applicable_phases,
            'related_memories': self.related_memories,
            'derived_from': self.derived_from,
        }


class PrismEngine:
    """
    The unified brain of PRISM.

    Single entry point for all memory and intelligence operations.
    NO async, NO wrappers, just direct calls to storage.
    """

    def __init__(self):
        """Initialize PRISM engine from configuration."""
        config = get_config()

        # Initialize storage
        self.embedder = get_embedder()
        self.qdrant = QdrantManager()
        self.neo4j = Neo4jManager()

        # Adaptive thresholds
        self.enable_adaptive_thresholds = (
            config.adaptive_thresholds.enable_adaptive_thresholds
        )
        self.default_threshold = config.adaptive_thresholds.default_threshold
        self._adaptive_thresholds: dict[str, float] = {}

        if self.enable_adaptive_thresholds:
            try:
                self._load_adaptive_thresholds()
                logger.info(
                    f'Loaded {len(self._adaptive_thresholds)} adaptive thresholds'
                )
            except Exception as e:
                logger.warning(
                    f'Failed to load adaptive thresholds: {e}\n'
                    f'Using default_threshold={self.default_threshold}'
                )

        # Promotion thresholds (from config, NO DEFAULTS)
        self.promotion_thresholds = {
            'to_anchors_frustration': config.memory.to_anchors_frustration,
            'to_longterm_accesses': config.memory.to_longterm_accesses,
            'to_episodic_age_hours': config.memory.to_episodic_age_hours,
            'working_ttl_hours': config.memory.working_ttl_hours,
        }

        # Importance weights (for graph-based promotion)
        self.importance_weights = {
            'graph': config.memory.importance_weights.graph,
            'access': config.memory.importance_weights.access,
            'frustration': config.memory.importance_weights.frustration,
        }

        # Initialize graph projection for PageRank
        self._ensure_graph_projection()

        logger.info('PrismEngine initialized')

    # ═══════════════════════════════════════════════════════════════════
    # MEMORY OPERATIONS
    # ═══════════════════════════════════════════════════════════════════

    def store_memory(
        self,
        content: str,
        memory_type: str,
        tier: Optional[MemoryTier] = None,
        frustration_score: float = 0.0,
        project_id: Optional[str] = None,
        branch: Optional[str] = None,
        session_id: Optional[str] = None,
        related_to: Optional[list[str]] = None,
        file_path: Optional[str] = None,
        symbol_name: Optional[str] = None,
        symbol_type: Optional[str] = None,
        line_start: Optional[int] = None,
        line_end: Optional[int] = None,
        signature: Optional[str] = None,
        git_sha: Optional[str] = None,
    ) -> Memory:
        """
        Store a new memory.

        Args:
            content: Memory content
            memory_type: Type (pattern, correction, decision, etc.)
            tier: Explicit tier (auto-assigned if None)
            frustration_score: User frustration (0-1)
            project_id: Project context
            branch: Branch context
            session_id: Session context
            related_to: Related memory IDs
            file_path: Code file path
            symbol_name: Code symbol name
            symbol_type: Code symbol type
            line_start: Code start line
            line_end: Code end line
            signature: Function signature
            git_sha: Git commit SHA

        Returns:
            Created Memory object
        """
        # Generate embedding
        embedding = self.embedder.generate_embedding(
            content, memory_type=memory_type
        )

        # Auto-assign tier if not specified
        if tier is None:
            tier = self._determine_tier(frustration_score, memory_type)

        # Create memory object
        memory = Memory(
            memory_id=self._generate_memory_id(),
            content=content,
            memory_type=memory_type,
            tier=tier,
            embedding=embedding,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            frustration_score=frustration_score,
            project_id=project_id,
            branch=branch,
            session_id=session_id,
            related_memories=related_to or [],
            file_path=file_path,
            symbol_name=symbol_name,
            symbol_type=symbol_type,
            line_start=line_start,
            line_end=line_end,
            signature=signature,
            git_sha=git_sha,
        )

        # Store in Qdrant for vector search
        self._store_in_qdrant(memory)

        # Store in Neo4j for graph ops and tracking
        self._store_in_neo4j(memory)

        # Store relationships if any
        if related_to:
            for related_id in related_to:
                rel = GraphRelationship(
                    from_id=memory.memory_id,
                    to_id=related_id,
                    relationship_type='RELATED_TO',
                    properties={'created_at': datetime.now().isoformat()},
                )
                self.neo4j.create_relationship(rel)

        logger.info(f'Stored memory {memory.memory_id} in {tier.value} tier')
        return memory

    def retrieve_memory(
        self,
        query: str,
        memory_type: str,
        limit: int = 10,
        tier_filter: Optional[MemoryTier] = None,
        project_id: Optional[str] = None,
        branch: Optional[str] = None,
        score_threshold: Optional[float] = None,
    ) -> list[Memory]:
        """
        Retrieve memories using semantic search.

        Args:
            query: Search query
            memory_type: Type of memory to retrieve
            limit: Maximum results
            tier_filter: Filter by tier
            project_id: Filter by project
            branch: Filter by branch
            score_threshold: Minimum similarity score

        Returns:
            List of relevant memories sorted by relevance
        """
        # Get tier-aware collection
        collection = self.qdrant.get_collection_for_memory_type(memory_type)

        # Generate query embedding
        query_embedding = self.embedder.generate_embedding(
            query, memory_type=memory_type
        )

        # Build filter conditions
        filter_conditions = {}
        if tier_filter:
            filter_conditions['tier'] = tier_filter.value
        if project_id:
            filter_conditions['project_id'] = project_id
        if branch:
            filter_conditions['branch'] = branch

        # Use adaptive threshold if enabled
        if score_threshold is None and self.enable_adaptive_thresholds:
            score_threshold = self._get_threshold_for_context(memory_type)

        # Search Qdrant
        results = self.qdrant.search(
            collection_name=collection,
            query_vector=query_embedding,
            limit=limit,
            filter_conditions=filter_conditions if filter_conditions else None,
            with_vectors=True,
            score_threshold=score_threshold,
        )

        # Convert to Memory objects
        memories = []
        for result in results:
            memory_data = result['payload']
            embedding_vector = result.get('vector', [])

            memory = Memory(
                memory_id=memory_data['memory_id'],
                content=memory_data['content'],
                memory_type=memory_data['memory_type'],
                tier=MemoryTier(memory_data['tier']),
                embedding=np.array(embedding_vector)
                if embedding_vector
                else np.array([]),
                created_at=datetime.fromisoformat(memory_data['created_at']),
                last_accessed=datetime.fromisoformat(
                    memory_data['last_accessed']
                ),
                access_count=memory_data['access_count'],
                frustration_score=memory_data['frustration_score'],
                similarity_score=result.get('score'),
                project_id=memory_data.get('project_id'),
                branch=memory_data.get('branch'),
                session_id=memory_data.get('session_id'),
                related_memories=memory_data['related_memories'],
                derived_from=memory_data.get('derived_from'),
                file_path=memory_data.get('file_path'),
                symbol_name=memory_data.get('symbol_name'),
                symbol_type=memory_data.get('symbol_type'),
                line_start=memory_data.get('line_start'),
                line_end=memory_data.get('line_end'),
                signature=memory_data.get('signature'),
                git_sha=memory_data.get('git_sha'),
            )
            memory.access()
            memories.append(memory)

        # Check for promotions based on usage
        for memory in memories:
            self._check_promotion(memory)

        return memories

    # ═══════════════════════════════════════════════════════════════════
    # ADR OPERATIONS
    # ═══════════════════════════════════════════════════════════════════

    def store_adr(
        self,
        decision: str,
        context: str,
        alternatives_considered: list[dict[str, str]],
        consequences: list[str],
        status: str,
        session_id: str,
        project_id: Optional[str] = None,
        related_symbols: Optional[list[str]] = None,
    ) -> str:
        """
        Store an architectural decision record.

        Args:
            decision: The decision made
            context: Context/problem being addressed
            alternatives_considered: List of alternatives with rejection reasons
            consequences: List of consequences
            status: Decision status (proposed, accepted, deprecated, superseded)
            session_id: Current session ID
            project_id: Optional project identifier
            related_symbols: Optional list of related code symbols

        Returns:
            ADR ID (UUID)
        """
        adr_id = str(uuid.uuid4())

        # Format ADR content
        content = self._format_adr(
            decision=decision,
            context=context,
            alternatives=alternatives_considered,
            consequences=consequences,
            status=status,
        )

        # Encode ADR metadata in content (with ADR-{id} prefix for filtering)
        enriched_content = (
            f'ADR-{adr_id}\n{content}\n\n'
            f'Metadata: status={status}, timestamp={datetime.utcnow().isoformat()}'
        )

        # Store in ANCHORS tier (ADRs never expire)
        self.store_memory(
            content=enriched_content,
            memory_type='decision',
            tier=MemoryTier.ANCHORS,
            frustration_score=0.0,
            project_id=project_id,
            session_id=session_id,
            related_to=related_symbols or [],
        )

        logger.info(f'Stored ADR {adr_id}: {decision}')
        return adr_id

    def query_adrs(
        self,
        query: str,
        session_id: str,
        project_id: Optional[str] = None,
        status: Optional[str] = None,
        limit: int = 10,
    ) -> list[dict[str, Any]]:
        """
        Query ADRs by semantic search.

        Args:
            query: Search query
            session_id: Current session ID
            project_id: Optional project filter
            status: Optional status filter (accepted, proposed, etc.)
            limit: Maximum results

        Returns:
            List of matching ADRs as dicts
        """
        # Retrieve from ANCHORS tier
        memories = self.retrieve_memory(
            query=query,
            memory_type='decision',
            limit=limit,
            tier_filter=MemoryTier.ANCHORS,
            project_id=project_id,
        )

        logger.debug(f'ADR query returned {len(memories)} raw memories')

        # Filter by ADR prefix (ensures we only get ADRs, not other decisions)
        results = []
        for mem in memories:
            if mem.content.startswith('ADR-'):
                results.append(mem.to_dict())

        logger.info(f'ADR query returning {len(results)} ADRs after filtering')
        return results

    def _format_adr(
        self,
        decision: str,
        context: str,
        alternatives: list[dict[str, str]],
        consequences: list[str],
        status: str,
    ) -> str:
        """Format ADR in standardized template."""
        lines = [
            f'# Architectural Decision: {decision}',
            '',
            f'**Status:** {status}',
            f'**Date:** {datetime.utcnow().strftime("%Y-%m-%d")}',
            '',
            '## Context',
            context,
            '',
            '## Decision',
            decision,
            '',
            '## Alternatives Considered',
        ]

        for alt in alternatives:
            lines.append(
                f'- **{alt["alternative"]}**: {alt["reason_rejected"]}'
            )

        lines.extend(['', '## Consequences'])

        for consequence in consequences:
            lines.append(f'- {consequence}')

        return '\n'.join(lines)

    # ═══════════════════════════════════════════════════════════════════
    # DUPLICATION DETECTION
    # ═══════════════════════════════════════════════════════════════════

    def detect_duplicates(
        self,
        project_path: str,
        min_lines: int = 10,
        similarity_threshold: float = 0.85,
    ) -> dict[str, Any]:
        """
        Detect code duplication across project.

        Args:
            project_path: Path to project directory
            min_lines: Minimum lines for a code block
            similarity_threshold: Minimum similarity (0.85 = 85%)

        Returns:
            Detection results with duplicate groups
        """
        from prism_mcp.core.duplication_detector import DuplicationDetector

        detector = DuplicationDetector(
            embedding_model=self.embedder,
            similarity_threshold=similarity_threshold,
        )

        duplicate_groups = detector.detect_project_duplicates(
            project_path=project_path, min_lines=min_lines
        )

        results = {
            'duplicate_groups': duplicate_groups,
            'duplicate_count': len(duplicate_groups),
        }

        logger.info(
            f'Duplication detection: {results.get("duplicate_count", 0)} duplicates found'
        )
        return results

    def get_tier_statistics(self) -> dict[str, Any]:
        """
        Get statistics about memory tiers.

        Returns:
            Tier counts and statistics
        """
        stats = {}

        # Query statistics from database
        for tier in MemoryTier:
            # Count memories in this tier
            tier_result = self.neo4j.query(
                """
                MATCH (m:Memory {tier: $tier})
                RETURN count(m) as count,
                       avg(m.access_count) as avg_access,
                       max(m.access_count) as max_access,
                       avg(m.frustration_score) as avg_frustration,
                       max(m.frustration_score) as max_frustration
            """,
                tier=tier.value,
            )

            if tier_result and tier_result[0]['count'] > 0:
                stats[tier.value] = {
                    'count': tier_result[0]['count'],
                    'avg_access_count': tier_result[0]['avg_access'] or 0,
                    'max_access_count': tier_result[0]['max_access'] or 0,
                    'avg_frustration': tier_result[0]['avg_frustration'] or 0,
                    'max_frustration': tier_result[0]['max_frustration'] or 0,
                }
            else:
                stats[tier.value] = {'count': 0}

        return stats

    def cleanup_stale_memories(self):
        """
        Clean up stale WORKING memories based on TTL.

        This runs periodically to prevent memory bloat.
        """
        from datetime import timedelta

        ttl_hours = self.promotion_thresholds['working_ttl_hours']
        cutoff_time = datetime.now() - timedelta(hours=ttl_hours)

        # Query stale memories from database
        stale_result = self.neo4j.query(
            """
            MATCH (m:Memory {tier: $tier})
            WHERE m.last_accessed < $cutoff
            RETURN m.memory_id as id
        """,
            tier=MemoryTier.WORKING.value,
            cutoff=cutoff_time.isoformat(),
        )

        stale_ids = [r['id'] for r in stale_result] if stale_result else []

        # Delete stale memories from databases only
        for memory_id in stale_ids:
            # Remove from Qdrant
            self.qdrant.delete_pattern(memory_id)
            # Remove from Neo4j
            self.neo4j.query(
                """
                MATCH (m:Memory {memory_id: $id})
                DELETE m
            """,
                id=memory_id,
            )

        if stale_ids:
            logger.info(f'Cleaned up {len(stale_ids)} stale WORKING memories')

    # ═══════════════════════════════════════════════════════════════════
    # INTERNAL HELPERS
    # ═══════════════════════════════════════════════════════════════════

    def _determine_tier(
        self, frustration_score: float, memory_type: str
    ) -> MemoryTier:
        """Determine initial tier based on frustration and patterns."""
        # Query Neo4j for similar memories' tier patterns
        result = self.neo4j.query(
            """
            MATCH (m:Memory)
            WHERE m.type = $memory_type
            WITH m LIMIT 1000
            WITH m.tier as tier, avg(m.frustration_score) as avg_frustration
            ORDER BY avg_frustration DESC
            RETURN tier, avg_frustration
            LIMIT 1
            """,
            memory_type=memory_type,
        )

        if result and frustration_score >= result[0]['avg_frustration']:
            return MemoryTier(result[0]['tier'])

        # Default: start in WORKING, let it promote naturally
        return MemoryTier.WORKING

    def _check_promotion(self, memory: Memory):
        """Check if memory should be promoted using graph importance."""
        try:
            # Get PageRank score from graph projection
            proj_exists = self.neo4j.query(
                "CALL gds.graph.exists('memory-graph') YIELD exists RETURN exists"
            )

            if proj_exists and proj_exists[0]['exists']:
                result = self.neo4j.query(
                    """
                    MATCH (m:Memory {memory_id: $memory_id})
                    CALL gds.pageRank.stream('memory-graph')
                    YIELD nodeId, score
                    WHERE id(m) = nodeId
                    RETURN score
                    """,
                    memory_id=memory.memory_id,
                )
                importance_score = result[0]['score'] if result else 0.1
            else:
                importance_score = 0.1
        except Exception as e:
            logger.warning(
                f'Graph projection error: {e}. Using simple importance.'
            )
            importance_score = 0.1

        # Calculate access rate relative to age
        age_hours = (datetime.now() - memory.created_at).total_seconds() / 3600
        access_rate = (
            memory.access_count / age_hours
            if age_hours > 0
            else memory.access_count
        )

        # Combined importance using configured weights
        combined_importance = (
            importance_score * self.importance_weights['graph']
            + min(access_rate / 10, 1.0) * self.importance_weights['access']
            + memory.frustration_score * self.importance_weights['frustration']
        )

        # Get natural tier boundaries from data
        tier_boundaries = self._get_tier_boundaries()

        # Promote if importance exceeds natural boundaries
        if combined_importance >= tier_boundaries['anchors']:
            if memory.tier != MemoryTier.ANCHORS:
                self._promote_memory(memory, MemoryTier.ANCHORS)
        elif combined_importance >= tier_boundaries['longterm']:
            if memory.tier != MemoryTier.LONGTERM:
                self._promote_memory(memory, MemoryTier.LONGTERM)
        elif combined_importance >= tier_boundaries['episodic']:
            if memory.tier != MemoryTier.EPISODIC:
                self._promote_memory(memory, MemoryTier.EPISODIC)

    def _promote_memory(self, memory: Memory, new_tier: MemoryTier):
        """Promote memory to higher tier."""
        old_tier = memory.tier
        memory.tier = new_tier
        self._update_in_qdrant(memory)
        logger.info(
            f'Promoted memory {memory.memory_id} from {old_tier.value} to {new_tier.value}'
        )

    def _get_tier_boundaries(self) -> dict[str, float]:
        """Get natural tier boundaries from data distribution using PageRank percentiles."""
        proj_exists = self.neo4j.query(
            "CALL gds.graph.exists('memory-graph') YIELD exists RETURN exists"
        )

        if proj_exists and proj_exists[0]['exists']:
            result = self.neo4j.query(
                """
                CALL gds.pageRank.stream('memory-graph')
                YIELD nodeId, score
                WITH collect(score) as scores
                RETURN percentileCont(scores, 0.95) as anchors_threshold,
                       percentileCont(scores, 0.80) as longterm_threshold,
                       percentileCont(scores, 0.50) as episodic_threshold
                """
            )

            if result:
                return {
                    'anchors': result[0]['anchors_threshold'],
                    'longterm': result[0]['longterm_threshold'],
                    'episodic': result[0]['episodic_threshold'],
                }

        # No graph yet - return high thresholds (nothing promotes)
        return {'anchors': 0.95, 'longterm': 0.80, 'episodic': 0.50}

    def _store_in_qdrant(self, memory: Memory):
        """Store memory in appropriate Qdrant collection (tier-aware)."""
        collection = self.qdrant.get_collection_for_memory_type(
            memory.memory_type
        )
        self.qdrant.ensure_collection(collection)

        payload = memory.to_dict()
        payload['collection'] = collection

        point = VectorPoint(
            point_id=self._memory_id_to_uuid(memory.memory_id),
            vector=memory.embedding,
            payload=payload,
        )

        self.qdrant.upsert(collection, [point])

    def _update_in_qdrant(self, memory: Memory):
        """Update memory in Qdrant after tier change."""
        self._store_in_qdrant(memory)

    def _store_in_neo4j(self, memory: Memory):
        """Store memory node in Neo4j for tracking and graph algorithms."""
        memory_node = GraphNode(
            node_id=memory.memory_id,
            node_type='Memory',
            properties={
                'memory_id': memory.memory_id,
                'content': memory.content[:200],  # Truncate for graph
                'type': memory.memory_type,
                'tier': memory.tier.value,
                'project': memory.project_id,
                'branch': memory.branch,
                'session_id': memory.session_id,
                'access_count': memory.access_count,
                'frustration_score': memory.frustration_score,
                'created_at': memory.created_at.isoformat(),
                'last_accessed': memory.last_accessed.isoformat(),
            },
        )
        self.neo4j.create_node(memory_node, unique_property='memory_id')

    def _ensure_graph_projection(self):
        """Ensure graph projection exists for GDS algorithms (PageRank)."""
        result = self.neo4j.query(
            "CALL gds.graph.exists('memory-graph') YIELD exists RETURN exists"
        )

        if result and result[0]['exists']:
            logger.debug("Graph projection 'memory-graph' exists")
        else:
            self._create_graph_projection()

    def _create_graph_projection(self):
        """Create graph projection for memory analysis (only if nodes + relationships exist)."""
        # Check if Memory nodes exist
        count_result = self.neo4j.query(
            'MATCH (m:Memory) RETURN count(m) as count'
        )
        node_count = count_result[0]['count'] if count_result else 0

        # Check if RELATED_TO relationships exist
        rel_result = self.neo4j.query(
            'MATCH ()-[r:RELATED_TO]->() RETURN count(r) as count'
        )
        rel_count = rel_result[0]['count'] if rel_result else 0

        if node_count == 0:
            logger.info('No Memory nodes yet - skipping graph projection')
            return

        if rel_count == 0:
            logger.info(
                f'Have {node_count} nodes but no relationships - skipping graph projection'
            )
            return

        # Drop existing projection
        self.neo4j.query(
            "CALL gds.graph.drop('memory-graph', false) YIELD graphName RETURN graphName"
        )

        # Create new projection
        result = self.neo4j.query(
            """
            CALL gds.graph.project(
                'memory-graph',
                'Memory',
                'RELATED_TO'
            )
            YIELD graphName, nodeCount, relationshipCount
            RETURN graphName, nodeCount, relationshipCount
            """
        )

        if not result:
            raise RuntimeError('Failed to create graph projection')

        logger.info(
            f'Created graph projection: {result[0]["nodeCount"]} nodes, '
            f'{result[0]["relationshipCount"]} relationships'
        )

    def _load_adaptive_thresholds(self):
        """Load adaptive thresholds from Neo4j."""
        from prism_mcp.core.threshold_learner import get_threshold_learner

        learner = get_threshold_learner()
        self._adaptive_thresholds = learner.get_learned_thresholds()
        logger.debug(
            f'Loaded adaptive thresholds: {list(self._adaptive_thresholds.keys())}'
        )

    def _get_threshold_for_context(self, memory_type: str) -> float:
        """Get threshold for context (uses adaptive if available)."""
        if not self.enable_adaptive_thresholds:
            return self.default_threshold

        # Determine model from memory_type
        e5_types = {'research_note', 'decision', 'gotcha', 'guidance'}
        context_key = 'e5' if memory_type in e5_types else 'starcoder'

        # Check for learned threshold
        if context_key in self._adaptive_thresholds:
            threshold = self._adaptive_thresholds[context_key]
            logger.debug(
                f'Using adaptive threshold for {context_key}: {threshold:.2f}'
            )
            return threshold

        logger.debug(
            f'No adaptive threshold for {context_key}, using default: {self.default_threshold:.2f}'
        )
        return self.default_threshold

    def _generate_memory_id(self) -> str:
        """Generate unique memory ID."""
        unique_string = f'memory_{time.time()}_{random.randint(0, 999999)}'
        return hashlib.md5(unique_string.encode()).hexdigest()[:12]

    def _memory_id_to_uuid(self, memory_id: str) -> str:
        """Convert memory_id to UUID format for Qdrant."""
        # If already UUID, return as-is
        if len(memory_id) == 36 and memory_id.count('-') == 4:
            return memory_id

        # Legacy 12-char format - pad to UUID
        if len(memory_id) != 12:
            raise ValueError(
                f'memory_id must be 12 or 36 characters, got {len(memory_id)}'
            )

        padded = memory_id + '0' * 20
        return f'{padded[0:8]}-{padded[8:12]}-{padded[12:16]}-{padded[16:20]}-{padded[20:32]}'


def get_prism_engine() -> PrismEngine:
    """Get PrismEngine instance (singleton)."""
    return PrismEngine()
