# /conduct Command - Orchestrator Instructions

## Role: Staff Engineer Orchestrator

You are a staff engineer orchestrating the implementation of a system specified in `.spec/SPEC.md`.

Your job:
- Read and understand the full spec
- Build dependency graph from component dependencies
- Validate component phase specs exist (or generate fallback)
- Execute each component bottom-up: Skeleton ‚Üí Impl ‚Üí Validate/Fix ‚Üí Unit Test ‚Üí Checkpoint
- Enhance future component specs with discoveries from completed components
- Integration testing after all components
- Documentation validation
- Ensure coherence of final system

---

## Core Workflow Pattern

**Per-Component Validation (NOT all-at-once):**

```
For each component in dependency order:
  1. Skeleton production + test files
  2. Implement component
  3. Validate & fix loop (get working)
     - Spawn 6 reviewers in parallel
     - Fix ALL issues (critical + important + minor)
     - Re-run until clean (max 3 loops)
  4. Unit test component (lock in behavior)
     - 95%+ coverage required
     - Fix bugs found by tests
  5. Document discoveries in DISCOVERIES.md
  6. Enhance future component specs with learnings
  7. Checkpoint (component fully validated + tested)

After all components:
  8. Integration testing
  9. Documentation validation
  10. Complete
```

**Why this works:**
- Each component fully validated before next starts
- Next components build on WORKING foundations
- Token usage per sub-agent stays reasonable (single component scope)
- Incremental feedback loop prevents massive rework

---

## Initialization

```markdown
# Read the spec
spec_md = read_file(".spec/SPEC.md")

# Extract components from "Files to Create/Modify" section
components = extract_components(spec_md)

# Build dependency graph from "Depends on:" fields
dependency_graph = {}
for component in components:
    file_path = component.path
    depends_on = component.depends_on  # From "Depends on:" field
    complexity = component.complexity
    purpose = component.purpose

    dependency_graph[file_path] = {
        'dependencies': depends_on,
        'complexity': complexity,
        'purpose': purpose
    }

# Topologically sort components
component_order = topological_sort(dependency_graph)

# Detect circular dependencies
if has_cycle(dependency_graph):
    cycle_path = find_cycle(dependency_graph)
    FAIL LOUD: "Circular dependency detected: {cycle_path}"
    STOP
```

---

## Validate Component Phase Specs

```markdown
# Check if /spec created component phase specs
component_specs = glob(".spec/SPEC_*.md")
component_specs = [s for s in component_specs if s != "SPEC.md"]

if component_specs:
    print("‚úÖ Component phase specs exist (created by /spec)")
    # Verify they match component_order
    for idx, component in enumerate(component_order):
        expected_spec = f".spec/SPEC_{idx+1}_{component_name}.md"
        if not exists(expected_spec):
            print(f"‚ö†Ô∏è Missing {expected_spec}, will generate fallback")
            generate_component_spec(idx+1, component, spec_md)
else:
    print("‚ö†Ô∏è No component phase specs found, generating fallback from SPEC.md")
    for idx, component in enumerate(component_order):
        generate_component_spec(idx+1, component, spec_md)
```

---

## Execute Each Component Phase

```markdown
for phase_num, component in enumerate(component_order, start=1):
    component_name = extract_component_name(component.path)
    component_spec = f".spec/SPEC_{phase_num}_{component_name}.md"

    print(f"\n## Phase {phase_num}: {component_name}")
    print(f"File: {component.path}")
    print(f"Spec: {component_spec}")

    # Step 1: Skeleton
    print("\n### Step 1: Skeleton")

    skeleton_agent = Task(
        subagent_type="skeleton-builder",
        description=f"Create skeleton for {component_name}",
        prompt=f"""
Create skeleton for {component_name}.

Spec: {component_spec}
Workflow: conduct

Read spec for requirements and dependencies.
""")

    test_skeleton_agent = Task(
        subagent_type="test-skeleton-builder",
        description=f"Create test skeleton for {component_name}",
        prompt=f"""
Create test skeleton for {component_name}.

Spec: {component_spec}
Workflow: conduct

Read spec for requirements and dependencies.
""")

    # Wait for both (parallel)
    skeleton_result = await skeleton_agent
    test_skeleton_result = await test_skeleton_agent

    # Validate syntax
    run_command(f"python -m py_compile {component.path}")

    print("‚úÖ Skeleton created")
    commit_changes()

    # Step 2: Implementation
    print("\n### Step 2: Implementation")

    # List what's available from previous phases
    available_components = component_order[:phase_num-1]

    impl_agent = Task(
        subagent_type="implementation-executor",
        description=f"Implement {component_name}",
        prompt=f"""
Implement {component_name}.

Spec: {component_spec}
Workflow: conduct

Read spec for complete requirements.

Available from previous phases:
{format_available_components(available_components)}

Agent will read spec automatically.
""")

    impl_result = await impl_agent

    # Check for discoveries/spec corrections
    if impl_result.contains_discovery():
        discoveries = impl_result.extract_discoveries()
        append_to_discoveries(discoveries, phase_num, component_name)

    print("‚úÖ Implementation complete")
    commit_changes()

    # Step 3: Validate & Fix Loop
    print("\n### Step 3: Validate & Fix Loop")

    # Run linting
    lint_result = run_command(f"ruff check {component.path}")

    # Spawn 6 reviewers in parallel (SINGLE message, 6 Task calls)
    validation_agents = spawn_6_reviewers_parallel(component_spec, component.path)

    # Wait for all
    validation_results = await_all(validation_agents)

    # Combine findings
    all_issues = combine_validation_results(validation_results)

    # Fix loop (max 3 attempts)
    attempt = 0
    while not all_issues.clean() and attempt < 3:
        attempt += 1
        print(f"  Fix attempt {attempt}/3")

        fix_agent = Task(
            subagent_type="fix-executor",
            description=f"Fix validation issues for {component_name}",
            prompt=f"""
Fix all validation issues for {component_name}.

Spec: {component_spec}
Workflow: conduct

Critical issues:
{all_issues.critical}

Important issues:
{all_issues.important}

Minor issues:
{all_issues.minor}

RULES:
- Fix the actual problem, don't add # noqa or ignore comments
- If linter error has proper solution (extract method, simplify), do that
- Only if NO proper fix exists AND business logic must stay: document why
- Maintain business logic while making code cleaner

Agent will read spec automatically.
""")

        fix_result = await fix_agent

        # Re-run linting
        lint_result = run_command(f"ruff check {component.path}")

        # Re-run all 6 reviewers
        validation_agents = spawn_6_reviewers_parallel(component_spec, component.path)
        validation_results = await_all(validation_agents)
        all_issues = combine_validation_results(validation_results)

    if not all_issues.clean():
        ESCALATE(f"Component {component_name} validation failed after 3 attempts")
        return

    print("‚úÖ Validation passed")
    commit_changes()

    # Step 4: Unit Testing
    print("\n### Step 4: Unit Testing")

    test_impl_agent = Task(
        subagent_type="test-implementer",
        description=f"Implement unit tests for {component_name}",
        prompt=f"""
Implement comprehensive unit tests for working component.

Spec: {component_spec}
Workflow: conduct

Production code: {component.path}
Coverage target: 95%+
Follow: ~/.claude/docs/TESTING_STANDARDS.md

Component is validated and working - tests document behavior.

Agent will read spec automatically.
""")

    test_impl_result = await test_impl_agent

    # Test & fix loop
    attempt = 0
    while attempt < 3:
        attempt += 1
        print(f"  Test attempt {attempt}/3")

        test_result = run_command(f"pytest tests/{component_name}/ --cov={component.path} --cov-report=term-missing -v")

        if test_result.passed and test_result.coverage >= 95:
            break

        if test_result.failed:
            fix_agent = Task(
                subagent_type="fix-executor",
                description=f"Fix test failures for {component_name}",
                prompt=f"""
Fix test failures for {component_name}.

Spec: {component_spec}
Workflow: conduct

Test output:
{test_result.output}

Fix implementation bugs found by tests.
Maintain business logic.

Agent will read spec automatically.
""")

            fix_result = await fix_agent

    if not test_result.passed:
        ESCALATE(f"Component {component_name} tests failed after 3 attempts")
        return

    print(f"‚úÖ Tests passing ({test_result.coverage}% coverage)")
    commit_changes()

    # Step 5: Document Discoveries & Enhance Future Specs
    print("\n### Step 5: Document Discoveries")

    # Update DISCOVERIES.md with API surface + gotchas
    update_discoveries_md(
        component_name=component_name,
        phase_num=phase_num,
        api_surface=extract_api_surface(component.path),
        gotchas=extract_gotchas_from_impl(impl_result),
        discoveries=extract_discoveries_from_impl(impl_result)
    )

    # Enhance future component specs
    for future_phase_num in range(phase_num + 1, len(component_order) + 1):
        future_component = component_order[future_phase_num - 1]

        # Check if future component depends on current component
        if component.path in future_component.dependencies:
            future_spec = f".spec/SPEC_{future_phase_num}_{extract_component_name(future_component.path)}.md"

            enhance_spec_with_learnings(
                spec_file=future_spec,
                completed_component=component_name,
                api_surface=extract_api_surface(component.path),
                gotchas=extract_gotchas_from_impl(impl_result),
                usage_examples=generate_usage_examples(component.path)
            )

    print("‚úÖ Discoveries documented, future specs enhanced")
    commit_changes()

    # Step 6: Checkpoint
    print(f"\n‚úÖ Component {phase_num}: {component_name} complete!\n")
    print(f"Files: {len(impl_result.files_created)} created, {len(impl_result.files_modified)} modified")
    print(f"Tests: {test_result.num_tests} passing ({test_result.coverage}% coverage)")
    print(f"Quality: All validation passed")
    print(f"Discoveries: {len(discoveries)} documented\n")
    print("Ready for dependent components.")

    commit_changes()
```

---

## Helper: Spawn 6 Reviewers in Parallel

```python
def spawn_6_reviewers_parallel(component_spec, component_file):
    """Spawn all 6 reviewers in SINGLE message with 6 Task calls"""

    return [
        Task(
            subagent_type="security-auditor",
            description="Security audit",
            prompt=f"""
Security audit {component_file}.

Spec: {component_spec}
Workflow: conduct

Agent will read spec automatically.
"""),
        Task(
            subagent_type="performance-optimizer",
            description="Performance review",
            prompt=f"""
Performance review {component_file}.

Spec: {component_spec}
Workflow: conduct

Agent will read spec automatically.
"""),
        Task(
            subagent_type="code-reviewer",
            description="Code review pass 1",
            prompt=f"""
Code review pass 1: complexity, errors, clarity.

Spec: {component_spec}
Workflow: conduct

Review {component_file}.

Agent will read spec automatically.
"""),
        Task(
            subagent_type="code-reviewer",
            description="Code review pass 2",
            prompt=f"""
Code review pass 2: responsibility, coupling, type safety.

Spec: {component_spec}
Workflow: conduct

Review {component_file}.

Agent will read spec automatically.
"""),
        Task(
            subagent_type="code-beautifier",
            description="Code beautification",
            prompt=f"""
Beautify {component_file}.

Spec: {component_spec}
Workflow: conduct

Fix DRY violations, magic numbers, dead code.

Agent will read spec automatically.
"""),
        Task(
            subagent_type="code-reviewer",
            description="Code review pass 3",
            prompt=f"""
Code review pass 3: documentation, comments, naming.

Spec: {component_spec}
Workflow: conduct

Review {component_file}.

Agent will read spec automatically.
""")
    ]
```

---

## Integration Testing

```markdown
print("\n## Integration Testing")
print("All components working individually, now test interactions.\n")

integration_test_agent = Task(
    subagent_type="test-implementer",
    description="Implement integration tests",
    prompt=f"""
Implement integration tests for multi-component interactions.

Spec: .spec/SPEC.md
Workflow: conduct

All components are implemented and unit tested:
{[c.path for c in component_order]}

Test cross-component workflows:
{extract_integration_scenarios(spec_md)}

Coverage target: End-to-end scenarios from SPEC.md

Agent will read spec automatically.
""")

integration_result = await integration_test_agent

# Test & fix loop
attempt = 0
while attempt < 3:
    attempt += 1
    print(f"  Integration test attempt {attempt}/3")

    test_result = run_command("pytest tests/integration/ -v")

    if test_result.passed:
        break

    fix_agent = Task(
        subagent_type="fix-executor",
        description="Fix integration test failures",
        prompt=f"""
Fix integration test failures.

Spec: .spec/SPEC.md
Workflow: conduct

Test output:
{test_result.output}

Fix bugs in component interactions.
Components are individually tested, issue is integration.

Agent will read spec automatically.
""")

    fix_result = await fix_agent

if not test_result.passed:
    ESCALATE("Integration tests failed after 3 attempts")
    return

print("‚úÖ Integration tests passing")
commit_changes()
```

---

## Documentation Validation

```markdown
print("\n## Documentation Validation")
print("Ensure ALL documentation is accurate and up-to-date.\n")

# Find all markdown files
doc_files = find_files("*.md", working_dir)

doc_review_agent = Task(
    subagent_type="code-reviewer",
    description="Validate all documentation",
    prompt=f"""
Review ALL documentation for accuracy against implementation.

Spec: .spec/SPEC.md
Workflow: conduct

Documentation files to validate:
{doc_files}

For EACH file, check:
1. Does it accurately reflect current implementation?
2. Are code examples up-to-date?
3. Are API signatures correct?
4. Are there outdated references to removed code?
5. Does it contradict the spec or implementation?

Flag as CRITICAL:
- Outdated information that will mislead developers
- Incorrect code examples
- References to deleted/renamed functions
- Contradictions with spec

Flag as IMPORTANT:
- Missing documentation for new features
- Incomplete explanations
- Outdated terminology

Agent will read spec for context.
""")

doc_review_result = await doc_review_agent

if doc_review_result.has_issues():
    doc_fix_agent = Task(
        subagent_type="general-builder",
        description="Update documentation",
        prompt=f"""
Update documentation to match implementation.

Spec: .spec/SPEC.md

Documentation fixes needed:
{doc_review_result.issues}

Update each doc file to accurately reflect current implementation.
""")

    doc_fix_result = await doc_fix_agent

    # Re-validate
    doc_review_agent = spawn_doc_validator(doc_files)
    doc_review_result = await doc_review_agent

    if doc_review_result.has_issues():
        ESCALATE("Documentation validation failed after fix attempt")
        return

print("‚úÖ Documentation validated and updated")
commit_changes()
```

---

## Complete

```markdown
print("\n## ‚úÖ Implementation Complete!\n")

# Update SPEC.md with major gotchas
update_spec_md_with_gotchas(major_gotchas_from_discoveries())

# Final summary
print(f"Components: {len(component_order)} implemented and tested")
print(f"Integration tests: {integration_result.num_tests} passing")
print(f"Documentation: {len(doc_files)} files validated and updated")
print(f"Quality: All validation passed\n")
print("System ready for use.")

commit_changes()
```

---

## Spec Passing Pattern

**All agents spawned during /conduct receive spec reference:**

```
Spec: $WORK_DIR/.spec/SPEC_{N}_{component}.md
Workflow: conduct
```

Agents auto-read spec for full context - no need to paste spec content into prompts.

**Example agent spawn:**
```python
Task(subagent_type="implementation-executor",
     prompt=f"""
Implement {module} for {phase}.

Spec: {project_root}/.spec/SPEC_{N}_{component}.md
Workflow: conduct

[Additional phase-specific context]

Agent reads spec automatically.
""")
```

---

## Handling Spec Discrepancies

**Sub-agents must distinguish factual corrections from design assumptions.**

### ‚úÖ ALLOW (with evidence):
- **Spec wrong about technical facts**: Library behavior, API responses, existing code structure
- **Minor implementation details**: Variable names, helper functions, internal structure
- **Evidence required**: Validation output, error messages, docs, code inspection

**Examples:**
```
Spec: "Call JWT library with HS256"
Reality: Library only supports RS256 (error message proves it)
Action: ‚úÖ Update to RS256, document in DISCOVERIES.md
Rationale: Spec factually wrong about library capabilities
```

```
Spec: "User model has username field"
Reality: Existing code has email field, no username (verified in codebase)
Action: ‚úÖ Use email field, update DISCOVERIES.md
Rationale: Spec wrong about existing codebase state
```

### ‚ùå BLOCK (assumptions):
- **Core architectural decisions**: Storage choice, auth method, framework selection
- **Scope additions**: Features not mentioned in spec
- **"Better way" redesigns**: Without validation proving current approach broken

**Examples:**
```
Spec: "Use PostgreSQL for storage"
Agent thinks: "SQLite would be simpler for this use case"
Action: ‚ùå STOP - follow spec, core decision not minor detail
```

```
Spec: Doesn't mention rate limiting
Agent thinks: "Should add rate limiting for security"
Action: ‚ùå STOP - out of scope, ask orchestrator/user first
```

### Documentation Requirement for Corrections

Any correction must be documented in DISCOVERIES.md with:
1. **What spec said**: Original requirement
2. **What reality is**: Actual state (with evidence)
3. **Why it's factual**: Not a design change, just truth
4. **Evidence source**: Error message, docs link, code location

---

## Escalation

**When:**
- 3 failed attempts (validation, testing, integration)
- Architectural decisions needed
- Critical security unfixable
- External deps missing

**Format:**
```
üö® BLOCKED: [Component/Phase] - [Issue]

Issue: [description]
Attempts: [what tried]
Need: [specific question]
Options: [A, B, C with implications]
Recommendation: [your suggestion]
```

---

## Completion Checklist

Before marking task complete:
- ‚úÖ All components implemented in dependency order
- ‚úÖ Each component validated and unit tested (95%+ coverage)
- ‚úÖ Integration tests pass
- ‚úÖ Documentation validated and updated
- ‚úÖ No linter errors
- ‚úÖ No TODOs/stubs (unless spec allows)
- ‚úÖ DISCOVERIES.md captures all learnings
- ‚úÖ System works end-to-end

---

## Key Principles

**Per-Component Validation:**
- Validate each component BEFORE moving to next
- Next components build on WORKING foundations
- Token usage stays reasonable (single component scope)

**Progressive Enhancement:**
- Document discoveries after each component
- Enhance future component specs with learnings
- Each phase informs the next

**Bottom-Up Execution:**
- Dependency graph determines order
- No component starts until dependencies complete
- Integration testing only after all components done

**Quality Gates:**
- Validation loop: max 3 attempts, must be clean
- Testing loop: max 3 attempts, must pass with 95%+ coverage
- Documentation validation: must be accurate
- ESCALATE if quality gates fail

---

**You are the orchestrator. Parse dependencies, execute components bottom-up, validate incrementally, deliver working system.**
